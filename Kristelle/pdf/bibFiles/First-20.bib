
@misc{noauthor_notitle_nodate,
}

@inproceedings{gorvadiya_energy_2025,
	address = {Gandhinagar, India},
	title = {Energy {Efficient} {Pruning} and {Quantization} {Methods} for {Deep} {Learning} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-2054-0},
	url = {https://ieeexplore.ieee.org/document/10932458/},
	doi = {10.1109/SETCOM64758.2025.10932458},
	abstract = {As deep learning models are increasingly being deployed on resource-constrained edge devices, the need to develop techniques to make the model more energy efficient without sacrificing its performance becomes crucial. In this study we explored the three most prominent approaches, such as bit quantization, model pruning, and adaptive switching techniques to address the computational and memory overheads of deep neural networks. Bit quantization reduces the weight and activation precisions, effectively lowering the hardware requirements for both storage and computation. Again the model pruning eliminates redundant parameters; thus, reduces the model complexity, and boost the inference time. Along with all these methods the adaptive switching techniques allow the model’s parameters or structure to switch at run-time so that trade-off between accuracy and energy consumption is dynamically optimized. In this paper we explored the advantages and limitations of such techniques for edge devices through a comprehensive study of recent advancements and methodologies. Point out that the open challenge on how to balance model performance with energy efficiency and future directions for the optimization of deep learning models for edge computing environments.},
	language = {en},
	urldate = {2025-03-28},
	booktitle = {2025 {International} {Conference} on {Sustainable} {Energy} {Technologies} and {Computational} {Intelligence} ({SETCOM})},
	publisher = {IEEE},
	author = {Gorvadiya, Jay and Chagela, Ankur and Roy, Mohendra},
	month = feb,
	year = {2025},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\H6XN8PYV\\Gorvadiya et al. - 2025 - Energy Efficient Pruning and Quantization Methods for Deep Learning Models.pdf:application/pdf},
}

@inproceedings{kim_fpga_2021,
	address = {Paris, France},
	title = {{FPGA} {Prototyping} of {Systolic} {Array}-based {Accelerator} for {Low}-{Precision} {Inference} of {Deep} {Neural} {Networks}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6956-2},
	url = {https://ieeexplore.ieee.org/document/9806200/},
	doi = {10.1109/RSP53691.2021.9806200},
	abstract = {In this study, we aim to design an energy-efﬁcient computation system for deep neural networks on edge devices. To maximize energy efﬁciency, we design a novel hardware accelerator that supports low-precision computation and sparsityaware structured zero-skipping on top of the well-known systolicarray structure. In addition, we introduce a full-stack software platform, including a model optimizer, instruction compiler, and host interface, to translate the pre-trained PyTorch model to the proposed accelerator and orchestrate it automatically. We validate the entire system by prototyping the accelerator on the Xilinx Alveo U250 FPGA board and demonstrating the inference of the 4-bit ResNet-50 model through the software stack. According to our experiment, our platform shows 317 GOPS inference speed and 51.96 GOPS/W energy efﬁciency for ResNet-50 on Xilinx Alveo U250 FPGA at 108 MHz, which is comparable to the advanced commercial acceleration system in terms of energy efﬁciency.},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {2021 {IEEE} {International} {Workshop} on {Rapid} {System} {Prototyping} ({RSP})},
	publisher = {IEEE},
	author = {Kim, Soobeom and Cho, Seunghwan and Park, Eunhyeok and Yoo, Sungjoo},
	month = oct,
	year = {2021},
	pages = {1--7},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\CMDGP9N6\\Kim et al. - 2021 - FPGA Prototyping of Systolic Array-based Accelerator for Low-Precision Inference of Deep Neural Netw.pdf:application/pdf},
}

@article{liu_energy-efficient_2024,
	title = {Energy-{Efficient} {Computing} {Acceleration} of {Unmanned} {Aerial} {Vehicles} {Based} on a {CPU}/{FPGA}/{NPU} {Heterogeneous} {System}},
	volume = {11},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/10525057/},
	doi = {10.1109/JIOT.2024.3397649},
	abstract = {The time and energy optimization of computationally intensive tasks involving unmanned air vehicles (UAVs) is highly important for increasing the reaction speed of UAVs and for prolonging their lifetime. To achieve the above objective, many studies based on heterogeneous computing have been carried out. Although these studies have achieved good results, limitations remain. First, neural processing units (NPUs) have emerged in recent years. However, insufﬁcient attention has been devoted to CPU/NPU research in academia currently. Second, most popular heterogeneous computing architectures have only one kind of accelerator, e.g., CPU/GPU or CPU/ﬁeld programmable gate array (FPGA). A heterogeneous system with multiple kinds of accelerators, e.g., CPU/FPGA/NPU, has not been investigated in depth. To address the above concerns, we propose a heterogeneous CPU/FPGA/NPU system aimed at realizing energy-efﬁcient computing acceleration for computationally intensive UAV tasks. First, we select several representative computationally intensive UAV tasks and design FPGA and NPU accelerators dedicated to these tasks. Then, we calculate the time and energy costs of these tasks on the FPGA and NPU, respectively, and ﬁnd that different tasks are appropriate for running on different cores. Based on this ﬁnding, we further build a heterogeneous CPU/FPGA/NPU architecture and assign each UAV task to the most appropriate core for execution. In this way, the UAV tasks can be executed more efﬁciently. We conduct experiments by executing all the representative UAV tasks on the CPU, CPU/GPU, CPU/FPGA, CPU/NPU and CPU/FPGA/NPU platforms. The results show that a heterogeneous system with multiple accelerators can achieve better computing performance and higher energy efﬁciency.},
	language = {en},
	number = {16},
	urldate = {2025-03-31},
	journal = {IEEE Internet of Things Journal},
	author = {Liu, Xing and Xu, Wenxing and Wang, Qing and Zhang, Mengya},
	month = aug,
	year = {2024},
	pages = {27126--27138},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\TCDCGNCT\\Liu et al. - 2024 - Energy-Efficient Computing Acceleration of Unmanned Aerial Vehicles Based on a CPUFPGANPU Heteroge.pdf:application/pdf},
}

@article{manor_custom_2022,
	title = {Custom {Hardware} {Inference} {Accelerator} for {TensorFlow} {Lite} for {Microcontrollers}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9825651/},
	doi = {10.1109/ACCESS.2022.3189776},
	abstract = {In recent years, the need for the efﬁcient deployment of Neural Networks (NN) on edge devices has been steadily increasing. However, the high computational demand required for Machine Learning (ML) inference on tiny microcontroller-based IoT devices avoids a direct software deployment on such resource-constrained edge devices. Therefore, various custom and application-speciﬁc NN hardware accelerators have been proposed to enable real-time Machine Learning (ML) inference on low-power and resource-limited edge devices. Efﬁcient mapping of the computational load onto hardware and software resources is a key challenge for performance improvement while keeping low power and a low area footprint. High performance and yet low power embedded processors may be attained via the usage of hardware acceleration. This paper presents an efﬁcient hardware-software framework to accelerate machine learning inference on edge devices using a modiﬁed TensorFlow Lite for Microcontroller (TFLM) model running on a Microcontroller (MCU) and a dedicated Neural Processing Unit (NPU) custom hardware accelerator, referred to as MCU-NPU. The proposed framework supports weight compression of pruned quantized NN models and exploits the pruned model sparsity to reduce computational complexity further. The proposed methodology has been evaluated by employing the MCU-NPU acceleration for various TFLM-based NN architectures using the common MLPerf Tiny benchmark. Experimental results demonstrate a signiﬁcant speedup of up to 724x compared to a pure software implementation. For example, the resulting runtime for the CIFAR-10 classiﬁcation is reduced from about 20 sec to only 37 ms using the proposed hardware acceleration. Moreover, the proposed hardware accelerator outperforms all the reference models optimized for edge devices in terms of inference runtime.},
	language = {en},
	urldate = {2025-03-31},
	journal = {IEEE Access},
	author = {Manor, Erez and Greenberg, Shlomo},
	year = {2022},
	pages = {73484--73493},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\CLBGJ4QU\\Manor and Greenberg - 2022 - Custom Hardware Inference Accelerator for TensorFlow Lite for Microcontrollers.pdf:application/pdf},
}

@inproceedings{qiao_energy_2023,
	address = {Changchun, China},
	title = {Energy {Efficiency} {Optimization} {Algorithm} {Based} on {CNN}-{SPP}-{AM} for {D2D} {Communication}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-1667-4},
	url = {https://ieeexplore.ieee.org/document/10435507/},
	doi = {10.1109/EIECS59936.2023.10435507},
	abstract = {The underlying multiplexed spectrum in D2D communications has an impact on spectral efficiency and multiplexing can introduce serious interference in communication systems. Many communication devices are subject to limited power supplies. Therefore, Therefore, we need to address the issue of resource allocation in D2D communication and maximise energy efficiency. Therefore, this paper proposes a CNN-SPP-AM based optimisation method for energy efficiency of D2D communication, which firstly pools the pooling layer of the convolutional neural network with spatial pyramids and uses a zero padding strategy in the output layer to remove the input size limitation. Then, the attention mechanism is introduced into the optimised convolutional neural network to improve the learning ability of individual features.The simulation results show that the proposed CNN-SPP-AM outperforms other neural networks in terms of system energy efficiency and system sum rate.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2023 3rd {International} {Conference} on {Electronic} {Information} {Engineering} and {Computer} {Science} ({EIECS})},
	publisher = {IEEE},
	author = {Qiao, Shixia and Zheng, Kaijin and Zhou, Jialu and Qu, Zitao},
	month = sep,
	year = {2023},
	pages = {289--292},
	annote = {No citations on mindmap - did not seem relevant to the project
},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\G68GLCKF\\Qiao et al. - 2023 - Energy Efficiency Optimization Algorithm Based on CNN-SPP-AM for D2D Communication.pdf:application/pdf},
}

@inproceedings{ks_modified_2022,
	address = {Malang, Indonesia},
	title = {Modified {CNN} to {Maximize} {Energy} {Efficiency} in {D2D} {Underlying} with {Multi}-{Cell} {Cellular} {Network}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-9742-8},
	url = {https://ieeexplore.ieee.org/document/9865637/},
	doi = {10.1109/CyberneticsCom55287.2022.9865637},
	abstract = {The usage of Device-to-Device (D2D) underlaying to reuse spectrum has a substantial inﬂuence on spectrum efﬁciency. On the other side, interference issues arise as a result of frequency reused by D2D users. Furthermore, wearable devices or communication devices have limited power sources, such as batteries. As a result, the fundamental problem formulation that must be solved is power allocation, with the goal function being to maximize the energy efﬁciency of the system. In order to provide optimum power allocation, conventional methods such as Convex Approximation (CA)based algorithm need to run multiple iterations to solve the non-convex problem formulation. Therefore, Convolution Neural Network (CNN) as part of Deep Learning (DL) is utilized to approach (CA)-based algorithm for generating power allocation policies to maximize the systems energy efﬁciency. However, the conventional method of CNN has limitations in accepting arbitrary input size. Accordingly, to the limitation of CNN, this research proposed the combination of CNN with Spatial Pyramid Pooling (SPP) to overcome the limitation on the input size of conventional CNN. Speciﬁcally, the inputs of the model are the user’s channel state information, and its outputs are power control policies. The simulation results show that both CNN-SPP and CNN can achieve similar performance to the traditional method up to 95\% accuracy. Furthermore, the combination of CNN and SPP can overcome the limitation on the input size of the conventional CNN method, reducing the number of models that must be trained to just one and applying it to all scenarios regardless of the number of CUEs D2D pairs.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cybernetics} and {Computational} {Intelligence} ({CyberneticsCom})},
	publisher = {IEEE},
	author = {K.S, Bayu Setho and Fahmi, Arfianto and Adriansyah, Nachwan Mufti and W. Prabowo, V. S.},
	month = jun,
	year = {2022},
	pages = {359--364},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\3FCMFQDA\\K.S et al. - 2022 - Modified CNN to Maximize Energy Efficiency in D2D Underlying with Multi-Cell Cellular Network.pdf:application/pdf},
}

@inproceedings{oh_investigation_2017,
	address = {Kuta Bali},
	title = {Investigation on performance and energy efficiency of {CNN}-based object detection on embedded device},
	isbn = {978-1-5386-0600-1},
	url = {http://ieeexplore.ieee.org/document/8320657/},
	doi = {10.1109/CAIPT.2017.8320657},
	abstract = {The use of a Convolutional Neural Network based method for object detection increases the accuracy that surpasses human visual system. Because it requires considerable computational capability, its use in embedded devices that place constraints in terms of power consumption as well as computational capability has thus far been limited. However, with the recent development of GPU for use in embedded devices and open-source software library for machine learning, it has become viable to utilize CNN in an energy-efficient embedded computing environment. In this study, CPU and GPU performance and energy efficiency of CNN-based object detection inference on an embedded platform is investigated through comparison with a traditional PC-based platform. Two publicly available hardware platforms are empirically evaluated; in one of them—NVIDIA Jetson TX-1—the results demonstrate image processing performance of 65\% of that of the PC, while the embedded device consumes 2.6\% of power consumed by the PC.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2017 4th {International} {Conference} on {Computer} {Applications} and {Information} {Processing} {Technology} ({CAIPT})},
	publisher = {IEEE},
	author = {Oh, Sangyoon and Kim, Minsub and Kim, Donghoon and Jeong, Minjoong and Lee, Minsu},
	month = aug,
	year = {2017},
	pages = {1--4},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\K5V7H78L\\Oh et al. - 2017 - Investigation on performance and energy efficiency of CNN-based object detection on embedded device.pdf:application/pdf},
}

@inproceedings{guha_secure_2024,
	address = {Knoxville, TN, USA},
	title = {Secure {Energy}-{Efficient} {Implementation} of {CNN} on {FPGAs} for {Accuracy} {Dependent} {Real} {Time} {Task} {Processing}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5411-9},
	url = {https://ieeexplore.ieee.org/document/10682707/},
	doi = {10.1109/ISVLSI61997.2024.00012},
	abstract = {A key objective of the fourth industrial revolution or Industry 4.0 is to use processing resources that are reconﬁgurable and ﬂexible, having additional capability to run smart real time intelligent applications in short time. For this, designers deploy reconﬁgurable hardware or ﬁeld programmable gate arrays (FPGAs) in several critical infrastructures, along with cloud and edge platforms. Among the commonly used artiﬁcial neural networks, convolutional neural networks (CNNs) is one of the most widely used for various smart applications. Existing strategies of CNN implementation on FPGAs incur signiﬁcant resources and power, and hence, are not energy efﬁcient. These are even prone to side channel attacks. Moreover, they are associated with prunning and hence, do not generate accurate results, which are important for some real time applications. In our proposed methodology, we pre-compute various operations, mainly operations associated with secret information, which in the present case are weights and bias of the CNN for a particular application and store in available embedded memory blocks (EMBs) of an FPGA. On demand, these pre-computed results are accessed for real time operations. Via this methodology, we eradicate the side channel attacks that steals the weights and biases, obtain low resource utilization, low latency, low power and better energy efﬁciency, without any loss of accuracy. Such a mechanism is particularly suitable for high accuracy real time smart intelligent applications.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2024 {IEEE} {Computer} {Society} {Annual} {Symposium} on {VLSI} ({ISVLSI})},
	publisher = {IEEE},
	author = {Guha, Krishnendu and Chakrabarti, Amlan},
	month = jul,
	year = {2024},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\T5N9FG58\\Guha and Chakrabarti - 2024 - Secure Energy-Efficient Implementation of CNN on FPGAs for Accuracy Dependent Real Time Task Process.pdf:application/pdf},
}

@inproceedings{noauthor_no_2017,
	address = {Guangzhou, China},
	title = {[{No} title found]},
	isbn = {978-1-5386-3790-6},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	publisher = {IEEE},
	month = dec,
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\RLLSGTP9\\2017 - [No title found].pdf:application/pdf},
}

@inproceedings{noauthor_no_2017-1,
	address = {Guangzhou, China},
	title = {[{No} title found]},
	isbn = {978-1-5386-3790-6},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	publisher = {IEEE},
	month = dec,
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\22ES3UXV\\2017 - [No title found].pdf:application/pdf},
}

@inproceedings{noauthor_no_2017-2,
	address = {Guangzhou, China},
	title = {[{No} title found]},
	isbn = {978-1-5386-3790-6},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	publisher = {IEEE},
	month = dec,
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\MA9PWBQ4\\2017 - [No title found].pdf:application/pdf},
}

@article{kim_energy-efficient_2020,
	title = {Energy-{Efficient} {Acceleration} of {Deep} {Neural} {Networks} on {Realtime}-{Constrained} {Embedded} {Edge} {Devices}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9262933/},
	doi = {10.1109/ACCESS.2020.3038908},
	abstract = {This paper presents a hardware management technique that enables energy-efﬁcient acceleration of deep neural networks (DNNs) on realtime-constrained embedded edge devices. It becomes increasingly common for edge devices to incorporate dedicated hardware accelerators for neural processing. The execution of neural accelerators in general follows a host-device model, where CPUs ofﬂoad neural computations (e.g., matrix and vector calculations) to the accelerators for datapath-optimized executions. Such a serialized execution is simple to implement and manage, but it is wasteful for the resource-limited edge devices to exercise only a single type of processing unit in a discrete execution phase. This paper presents a hardware management technique named NeuroPipe that utilizes heterogeneous processing units in an embedded edge device to accelerate DNNs in energy-efﬁcient manner. In particular, NeuroPipe splits a neural network into groups of consecutive layers and pipelines their executions using different types of processing units. The proposed technique offers several advantages to accelerate DNN inference in the embedded edge device. It enables the embedded processor to operate at lower voltage and frequency to enhance energy efﬁciency while delivering the same performance as uncontrolled baseline executions, or inversely it can dispatch faster inferences at the same energy consumption. Our measurement-driven experiments based on NVIDIA Jetson AGX Xavier with 64 tensor cores and eight-core ARM CPU demonstrate that NeuroPipe reduces energy consumption by 11.4\% on average without performance degradation, or it can achieve 30.5\% greater performance for the same energy consumption.},
	language = {en},
	urldate = {2025-03-31},
	journal = {IEEE Access},
	author = {Kim, Bogil and Lee, Sungjae and Trivedi, Amit Ranjan and Song, William J.},
	year = {2020},
	pages = {216259--216270},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\XMUPQR64\\Kim et al. - 2020 - Energy-Efficient Acceleration of Deep Neural Networks on Realtime-Constrained Embedded Edge Devices.pdf:application/pdf},
}

@article{que_remarn_2023,
	title = {Remarn: {A} {Reconfigurable} {Multi}-threaded {Multi}-core {Accelerator} for {Recurrent} {Neural} {Networks}},
	volume = {16},
	issn = {1936-7406, 1936-7414},
	shorttitle = {Remarn},
	url = {https://dl.acm.org/doi/10.1145/3534969},
	doi = {10.1145/3534969},
	abstract = {This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0\% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.},
	language = {en},
	number = {1},
	urldate = {2025-03-31},
	journal = {ACM Transactions on Reconfigurable Technology and Systems},
	author = {Que, Zhiqiang and Nakahara, Hiroki and Fan, Hongxiang and Li, He and Meng, Jiuxi and Tsoi, Kuen Hung and Niu, Xinyu and Nurvitadhi, Eriko and Luk, Wayne},
	month = mar,
	year = {2023},
	pages = {1--26},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\YF486GAK\\Que et al. - 2023 - Remarn A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks.pdf:application/pdf},
}

@article{kim_cnn_2023,
	title = {A {CNN} {Inference} {Accelerator} on {FPGA} {With} {Compression} and {Layer}-{Chaining} {Techniques} for {Style} {Transfer} {Applications}},
	volume = {70},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	url = {https://ieeexplore.ieee.org/document/10013941/},
	doi = {10.1109/TCSI.2023.3234640},
	abstract = {Recently, convolutional neural networks (CNNs) have actively been applied to computer vision applications such as style transfer that changes the style of a content image into that of a style image. As the style transfer CNNs are based on encoder-decoder network architecture and should deal with high-resolution images that become mainstream these days, the computational complexity and the feature map size are very large, preventing the CNNs from being implemented on an FPGA. This paper proposes a CNN inference accelerator for the style transfer applications, which employs network compression and layer-chaining techniques. The network compression technique is to make a style transfer CNN have low computational complexity and a small amount of parameters, and an efﬁcient data compression method is proposed to reduce the feature map size. In addition, the layer-chaining technique is proposed to reduce the off-chip memory trafﬁc and thus to increase the throughput at the cost of small hardware resources. In the proposed hardware architecture, a neural processing unit is designed by taking into account the proposed data compression and layer-chaining techniques. A prototype accelerator implemented on a FPGA board achieves a throughput comparable to the state-of-the-art accelerators developed for encoder-decoder CNNs.},
	language = {en},
	number = {4},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Kim, Suchang and Jang, Boseon and Lee, Jaeyoung and Bae, Hyungjoon and Jang, Hyejung and Park, In-Cheol},
	month = apr,
	year = {2023},
	pages = {1591--1604},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\V887VXBX\\Kim et al. - 2023 - A CNN Inference Accelerator on FPGA With Compression and Layer-Chaining Techniques for Style Transfe.pdf:application/pdf},
}

@inproceedings{esmaeilzadeh_neural_2012,
	address = {Vancouver, BC, Canada},
	title = {Neural {Acceleration} for {General}-{Purpose} {Approximate} {Programs}},
	isbn = {978-1-4673-4819-5 978-0-7695-4924-8},
	url = {http://ieeexplore.ieee.org/document/6493641/},
	doi = {10.1109/MICRO.2012.48},
	abstract = {This paper describes a learning-based approach to the acceleration of approximate programs. We describe the Parrot transformation, a program transformation that selects and trains a neural network to mimic a region of imperative code. After the learning phase, the compiler replaces the original code with an invocation of a low-power accelerator called a neural processing unit (NPU). The NPU is tightly coupled to the processor pipeline to accelerate small code regions. Since neural networks produce inherently approximate results, we deﬁne a programming model that allows programmers to identify approximable code regions—code that can produce imprecise but acceptable results. Ofﬂoading approximable code regions to NPUs is faster and more energy efﬁcient than executing the original code. For a set of diverse applications, NPU acceleration provides wholeapplication speedup of 2.3× and energy savings of 3.0× on average with quality loss of at most 9.6\%.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2012 45th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {IEEE},
	author = {Esmaeilzadeh, Hadi and Sampson, Adrian and Ceze, Luis and Burger, Doug},
	month = dec,
	year = {2012},
	pages = {449--460},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\ESC9HRFT\\Esmaeilzadeh et al. - 2012 - Neural Acceleration for General-Purpose Approximate Programs.pdf:application/pdf},
}

@article{wang_pl-npu_2022,
	title = {{PL}-{NPU}: {An} {Energy}-{Efficient} {Edge}-{Device} {DNN} {Training} {Processor} {With} {Posit}-{Based} {Logarithm}-{Domain} {Computing}},
	volume = {69},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	shorttitle = {{PL}-{NPU}},
	url = {https://ieeexplore.ieee.org/document/9803862/},
	doi = {10.1109/TCSI.2022.3184115},
	abstract = {Edge device deep neural network (DNN) training is practical to improve model adaptivity for unfamiliar datasets while avoiding privacy disclosure and huge communication cost. Nevertheless, apart from feed-forward (FF) as inference, DNN training still requires back-propagation (BP) and weight gradient (WG), introducing power-consuming ﬂoating-point computing requirements, hardware underutilization, and energy bottleneck from excessive memory access. This paper proposes a DNN training processor named PL-NPU to solve the above challenges with three innovations. First, a posit-based logarithmdomain processing element (PE) adapts to various training data requirements with a low bit-width format and reduces energy by transferring complicated arithmetics into simple logarithm domain operation. Second, a reconﬁgurable inter-intra-channelreuse dataﬂow dynamically adjusts the PE mapping with a regrouping omega network to improve the operands reuse for higher hardware utilization. Third, a pointed-stake-shaped codec unit adaptively compresses small values to variable-length data format while compressing large values to ﬁxed-length 8b posit format, reducing the memory access for breaking the training energy bottleneck. Simulated with 28nm CMOS technology, the proposed PL-NPU achieves a maximum frequency of 1040MHz with 343mW and 5.28mm2. The peak energy efﬁciency is 3.87TFLOPS/W for 0.6V at 60MHz. Compared with the state-ofthe-art training processor, PL-NPU reaches 3.75× higher energy efﬁciency and offers 1.68× speedup when training ResNet18.},
	language = {en},
	number = {10},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Wang, Yang and Deng, Dazheng and Liu, Leibo and Wei, Shaojun and Yin, Shouyi},
	month = oct,
	year = {2022},
	pages = {4042--4055},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\PQYSRZIT\\Wang et al. - 2022 - PL-NPU An Energy-Efficient Edge-Device DNN Training Processor With Posit-Based Logarithm-Domain Com.pdf:application/pdf},
}

@inproceedings{aliyev_pulse_2024,
	address = {Singapore, Singapore},
	title = {{PULSE}: {Parametric} {Hardware} {Units} for {Low}-power {Sparsity}-{Aware} {Convolution} {Engine}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-3099-1},
	shorttitle = {{PULSE}},
	url = {https://ieeexplore.ieee.org/document/10558062/},
	doi = {10.1109/ISCAS58744.2024.10558062},
	abstract = {Spiking Neural Networks (SNNs) have become popular for their more bio-realistic behavior than Artificial Neural Networks (ANNs). However, effectively leveraging the intrinsic, unstructured sparsity of SNNs in hardware is challenging, especially due to the variability in sparsity across network layers. This variability depends on several factors, including the input dataset, encoding scheme, and neuron model. Most existing SNN accelerators fail to account for the layer-specific workloads of an application (model + dataset), leading to high energy consumption. To address this, we propose a design-time parametric hardware generator that takes layer-wise sparsity and the number of processing elements as inputs and synthesizes the corresponding hardware. The proposed design compresses sparse spike trains using a priority encoder and efficiently shifts the activations across the network’s layers. We demonstrate the robustness of our proposed approach by first profiling a given application’s characteristics followed by performing efficient resource allocation. Results on a Xilinx Kintex FPGA (Field Programmable Gate Arrays) using MNIST, FashionMNIST, and SVHN datasets show a 3.14× improvement in accelerator efficiency (FPS/W) compared to a sparsity-oblivious systolic arraybased accelerator. Compared to the most recent sparsity-aware work, our solution improves efficiency by 1.72×.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2024 {IEEE} {International} {Symposium} on {Circuits} and {Systems} ({ISCAS})},
	publisher = {IEEE},
	author = {Aliyev, Ilkin and Adegbija, Tosiron},
	month = may,
	year = {2024},
	pages = {1--5},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\FR5LT8XF\\Aliyev and Adegbija - 2024 - PULSE Parametric Hardware Units for Low-power Sparsity-Aware Convolution Engine.pdf:application/pdf},
}

@article{que_remarn_2023-1,
	title = {Remarn: {A} {Reconfigurable} {Multi}-threaded {Multi}-core {Accelerator} for {Recurrent} {Neural} {Networks}},
	volume = {16},
	issn = {1936-7406, 1936-7414},
	shorttitle = {Remarn},
	url = {https://dl.acm.org/doi/10.1145/3534969},
	doi = {10.1145/3534969},
	abstract = {This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0\% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.},
	language = {en},
	number = {1},
	urldate = {2025-04-01},
	journal = {ACM Transactions on Reconfigurable Technology and Systems},
	author = {Que, Zhiqiang and Nakahara, Hiroki and Fan, Hongxiang and Li, He and Meng, Jiuxi and Tsoi, Kuen Hung and Niu, Xinyu and Nurvitadhi, Eriko and Luk, Wayne},
	month = mar,
	year = {2023},
	pages = {1--26},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\DYRZJUPG\\Que et al. - 2023 - Remarn A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks.pdf:application/pdf},
}

@inproceedings{noauthor_no_2017-3,
	address = {Guangzhou, China},
	title = {[{No} title found]},
	isbn = {978-1-5386-3790-6},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	publisher = {IEEE},
	month = dec,
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\EQPXMVEI\\2017 - [No title found].pdf:application/pdf},
}

@inproceedings{guha_secure_2024-1,
	address = {Knoxville, TN, USA},
	title = {Secure {Energy}-{Efficient} {Implementation} of {CNN} on {FPGAs} for {Accuracy} {Dependent} {Real} {Time} {Task} {Processing}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5411-9},
	url = {https://ieeexplore.ieee.org/document/10682707/},
	doi = {10.1109/ISVLSI61997.2024.00012},
	abstract = {A key objective of the fourth industrial revolution or Industry 4.0 is to use processing resources that are reconﬁgurable and ﬂexible, having additional capability to run smart real time intelligent applications in short time. For this, designers deploy reconﬁgurable hardware or ﬁeld programmable gate arrays (FPGAs) in several critical infrastructures, along with cloud and edge platforms. Among the commonly used artiﬁcial neural networks, convolutional neural networks (CNNs) is one of the most widely used for various smart applications. Existing strategies of CNN implementation on FPGAs incur signiﬁcant resources and power, and hence, are not energy efﬁcient. These are even prone to side channel attacks. Moreover, they are associated with prunning and hence, do not generate accurate results, which are important for some real time applications. In our proposed methodology, we pre-compute various operations, mainly operations associated with secret information, which in the present case are weights and bias of the CNN for a particular application and store in available embedded memory blocks (EMBs) of an FPGA. On demand, these pre-computed results are accessed for real time operations. Via this methodology, we eradicate the side channel attacks that steals the weights and biases, obtain low resource utilization, low latency, low power and better energy efﬁciency, without any loss of accuracy. Such a mechanism is particularly suitable for high accuracy real time smart intelligent applications.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2024 {IEEE} {Computer} {Society} {Annual} {Symposium} on {VLSI} ({ISVLSI})},
	publisher = {IEEE},
	author = {Guha, Krishnendu and Chakrabarti, Amlan},
	month = jul,
	year = {2024},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\8H3WET7L\\Guha and Chakrabarti - 2024 - Secure Energy-Efficient Implementation of CNN on FPGAs for Accuracy Dependent Real Time Task Process.pdf:application/pdf},
}

@inproceedings{duk_kim_24_2020,
	address = {San Francisco, CA, USA},
	title = {2.4 {A} 7nm {High}-{Performance} and {Energy}-{Efficient} {Mobile} {Application} {Processor} with {Tri}-{Cluster} {CPUs} and a {Sparsity}-{Aware} {NPU}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-3205-1},
	url = {https://ieeexplore.ieee.org/document/9062907/},
	doi = {10.1109/ISSCC19947.2020.9062907},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2020 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} - ({ISSCC})},
	publisher = {IEEE},
	author = {Duk Kim, Young and Jeong, Wookyeong and Jung, Lakkyung and Shin, Dongsuk and Song, Jae Geun and Song, Jinook and Kwon, Hyeokman and Lee, Jaeyoung and Jung, Jaesu and Kang, Myungjin and Jeong, Jaehun and Kwon, Yoonjoo and Seong, Nak Hee},
	month = feb,
	year = {2020},
	pages = {48--50},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\L53PA4NR\\Duk Kim et al. - 2020 - 2.4 A 7nm High-Performance and Energy-Efficient Mobile Application Processor with Tri-Cluster CPUs a.pdf:application/pdf},
}

@inproceedings{park_95_2021,
	address = {San Francisco, CA, USA},
	title = {9.5 {A} {6K}-{MAC} {Feature}-{Map}-{Sparsity}-{Aware} {Neural} {Processing} {Unit} in 5nm {Flagship} {Mobile} {SoC}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-9549-0},
	url = {https://ieeexplore.ieee.org/document/9365928/},
	doi = {10.1109/ISSCC42613.2021.9365928},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2021 {IEEE} {International} {Solid}- {State} {Circuits} {Conference} ({ISSCC})},
	publisher = {IEEE},
	author = {Park, Jun-Seok and Jang, Jun-Woo and Lee, Heonsoo and Lee, Dongwoo and Lee, Sehwan and Jung, Hanwoong and Lee, Seungwon and Kwon, Suknam and Jeong, Kyungah and Song, Joon-Ho and Lim, SukHwan and Kang, Inyup},
	month = feb,
	year = {2021},
	pages = {152--154},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\K7JXL8X8\\Park et al. - 2021 - 9.5 A 6K-MAC Feature-Map-Sparsity-Aware Neural Processing Unit in 5nm Flagship Mobile SoC.pdf:application/pdf},
}

@article{kim_cnn_2023-1,
	title = {A {CNN} {Inference} {Accelerator} on {FPGA} {With} {Compression} and {Layer}-{Chaining} {Techniques} for {Style} {Transfer} {Applications}},
	volume = {70},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	url = {https://ieeexplore.ieee.org/document/10013941/},
	doi = {10.1109/TCSI.2023.3234640},
	abstract = {Recently, convolutional neural networks (CNNs) have actively been applied to computer vision applications such as style transfer that changes the style of a content image into that of a style image. As the style transfer CNNs are based on encoder-decoder network architecture and should deal with high-resolution images that become mainstream these days, the computational complexity and the feature map size are very large, preventing the CNNs from being implemented on an FPGA. This paper proposes a CNN inference accelerator for the style transfer applications, which employs network compression and layer-chaining techniques. The network compression technique is to make a style transfer CNN have low computational complexity and a small amount of parameters, and an efﬁcient data compression method is proposed to reduce the feature map size. In addition, the layer-chaining technique is proposed to reduce the off-chip memory trafﬁc and thus to increase the throughput at the cost of small hardware resources. In the proposed hardware architecture, a neural processing unit is designed by taking into account the proposed data compression and layer-chaining techniques. A prototype accelerator implemented on a FPGA board achieves a throughput comparable to the state-of-the-art accelerators developed for encoder-decoder CNNs.},
	language = {en},
	number = {4},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Kim, Suchang and Jang, Boseon and Lee, Jaeyoung and Bae, Hyungjoon and Jang, Hyejung and Park, In-Cheol},
	month = apr,
	year = {2023},
	pages = {1591--1604},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\TXZHTXRC\\Kim et al. - 2023 - A CNN Inference Accelerator on FPGA With Compression and Layer-Chaining Techniques for Style Transfe.pdf:application/pdf},
}

@inproceedings{ito_power-efficient_2016,
	address = {Yokohama, Japan},
	title = {A power-efficient {FPGA} accelerator: {Systolic} array with cache-coherent interface for pair-{HMM} algorithm},
	isbn = {978-1-5090-1386-9},
	shorttitle = {A power-efficient {FPGA} accelerator},
	url = {http://ieeexplore.ieee.org/document/7503681/},
	doi = {10.1109/CoolChips.2016.7503681},
	abstract = {A systolic array is known as a parallel hardware architecture applicable to a wide range of applications. Naive implementations, however, can lead to inefficient resource usage and low power performance. In this paper, we discuss two techniques for improving the hardware resource usage: flexible multi-threading and dummy data padding. The design was implemented to accelerate a pair-HMM algorithm on an FPGA with the IBM POWER8 CAPI (Coherent Accelerator Processor Interface) feature. The CAPI feature simplifies the software design for driving the FPGA accelerator. Our experimental result indicates that the implemented FPGA accelerator executing the pair-HMM algorithm achieves 33x higher power performance than a POWER8 processor chip executing the same algorithm.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2016 {IEEE} {Symposium} in {Low}-{Power} and {High}-{Speed} {Chips} ({COOL} {CHIPS} {XIX})},
	publisher = {IEEE},
	author = {Ito, Megumi and Ohara, Moriyoshi},
	month = apr,
	year = {2016},
	pages = {1--3},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\KMJF56B2\\Ito and Ohara - 2016 - A power-efficient FPGA accelerator Systolic array with cache-coherent interface for pair-HMM algori.pdf:application/pdf},
}

@inproceedings{que_reconfigurable_2020,
	address = {Maui, HI, USA},
	title = {A {Reconfigurable} {Multithreaded} {Accelerator} for {Recurrent} {Neural} {Networks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-2302-1},
	url = {https://ieeexplore.ieee.org/document/9415593/},
	doi = {10.1109/ICFPT51103.2020.00012},
	abstract = {Recurrent Neural Network (RNN) is a key technology for sequential applications which require efﬁcient and realtime implementations. Despite its popularity, efﬁcient acceleration for RNN inference is challenging due to its recurrent nature and data dependencies. This paper proposes a multi-threaded neural processing unit (NPU) for RNN/LSTM inferences to increase processing abilities and quality of service of cloud-based NPUs by improving their hardware utilization. Besides, a custom coarse-grained multi-threaded LSTM (CGMT-LSTM) hardware architecture is introduced, which switches tasks among threads when LSTM computational kernels meet data hazard. These logical NPUs share nearly all resources of the physical NPU. When one logical NPU is stalled, another one can make progress. These optimizations improve the exploitation of parallelism to increase hardware utilization and enhance system throughput. Evaluation results show that a dual-threaded CGMT-LSTM NPU gains 27\% more performance while only has 3.8\% more area than a single-threaded one using a Stratix 10 FPGA. When compared with an implementation on the Tesla V100 GPU, our novel hardware architecture is 6.62 times faster and 15.88 times higher power efﬁciency, which demonstrates that our approach contributes to high performance energy-efﬁcient FPGA-based multi-LSTM inference systems.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2020 {International} {Conference} on {Field}-{Programmable} {Technology} ({ICFPT})},
	publisher = {IEEE},
	author = {Que, Zhiqiang and Nakahara, Hiroki and Fan, Hongxiang and Meng, Jiuxi and Tsoi, Kuen Hung and Niu, Xinyu and Nurvitadhi, Eriko and Luk, Wayne},
	month = dec,
	year = {2020},
	pages = {20--28},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\86S2EWC8\\Que et al. - 2020 - A Reconfigurable Multithreaded Accelerator for Recurrent Neural Networks.pdf:application/pdf},
}

@article{manor_custom_2022-1,
	title = {Custom {Hardware} {Inference} {Accelerator} for {TensorFlow} {Lite} for {Microcontrollers}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9825651/},
	doi = {10.1109/ACCESS.2022.3189776},
	abstract = {In recent years, the need for the efﬁcient deployment of Neural Networks (NN) on edge devices has been steadily increasing. However, the high computational demand required for Machine Learning (ML) inference on tiny microcontroller-based IoT devices avoids a direct software deployment on such resource-constrained edge devices. Therefore, various custom and application-speciﬁc NN hardware accelerators have been proposed to enable real-time Machine Learning (ML) inference on low-power and resource-limited edge devices. Efﬁcient mapping of the computational load onto hardware and software resources is a key challenge for performance improvement while keeping low power and a low area footprint. High performance and yet low power embedded processors may be attained via the usage of hardware acceleration. This paper presents an efﬁcient hardware-software framework to accelerate machine learning inference on edge devices using a modiﬁed TensorFlow Lite for Microcontroller (TFLM) model running on a Microcontroller (MCU) and a dedicated Neural Processing Unit (NPU) custom hardware accelerator, referred to as MCU-NPU. The proposed framework supports weight compression of pruned quantized NN models and exploits the pruned model sparsity to reduce computational complexity further. The proposed methodology has been evaluated by employing the MCU-NPU acceleration for various TFLM-based NN architectures using the common MLPerf Tiny benchmark. Experimental results demonstrate a signiﬁcant speedup of up to 724x compared to a pure software implementation. For example, the resulting runtime for the CIFAR-10 classiﬁcation is reduced from about 20 sec to only 37 ms using the proposed hardware acceleration. Moreover, the proposed hardware accelerator outperforms all the reference models optimized for edge devices in terms of inference runtime.},
	language = {en},
	urldate = {2025-04-01},
	journal = {IEEE Access},
	author = {Manor, Erez and Greenberg, Shlomo},
	year = {2022},
	pages = {73484--73493},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\28G9M5SP\\Manor and Greenberg - 2022 - Custom Hardware Inference Accelerator for TensorFlow Lite for Microcontrollers.pdf:application/pdf},
}

@article{choi_enabling_2023,
	title = {Enabling {Fine}-{Grained} {Spatial} {Multitasking} on {Systolic}-{Array} {NPUs} {Using} {Dataflow} {Mirroring}},
	volume = {72},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9340, 1557-9956, 2326-3814},
	url = {https://ieeexplore.ieee.org/document/10198513/},
	doi = {10.1109/TC.2023.3299030},
	abstract = {Neural Processing Units (NPUs) frequently suffer from low hardware utilization as the efﬁciency of their systolic arrays heavily depends on the characteristics of a deep neural network (DNN). Spatial multitasking is a promising solution to overcome the low NPU hardware utilization; however, the state-of-the-art spatial-multitasking NPU architecture achieves sub-optimal performance due to its coarse-grained systolic-array distribution and incurs signiﬁcant implementation costs. In this paper, we propose dataﬂow-mirroring NPU (DM-NPU), a novel spatial-multitasking NPU architecture supporting ﬁne-grained systolic-array distribution. The key idea of DM-NPU is to reverse the dataﬂows of co-located DNNs in horizontal and/or vertical directions. DM-NPU can place allocation boundaries between any adjacent processing elements of a systolic array, both horizontally and vertically. We then propose DM-Perf, an accurate systolic-array NPU performance model, to maximize the spatial-multitasking performance of DM-NPU. Utilizing the existing performance models achieves sub-optimal performance as they cannot accurately capture the resource contention caused by spatial multitasking. DM-Perf, on the other hand, exploits the per-layer performance proﬁles of a DNN to accurately capture the resource contention. Our evaluation using MLPerf DNNs shows that DM-NPU and DM-Perf can greatly improve the performance by up to 35.1\% over the state-of-the-art NPU architecture and performance model.},
	language = {en},
	number = {12},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Computers},
	author = {Choi, Jinwoo and Ha, Yeonan and Lee, Jounghoo and Lee, Sangsu and Lee, Jinho and Jang, Hanhwi and Kim, Youngsok},
	month = dec,
	year = {2023},
	pages = {3383--3398},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\WK3KXUDE\\Choi et al. - 2023 - Enabling Fine-Grained Spatial Multitasking on Systolic-Array NPUs Using Dataflow Mirroring.pdf:application/pdf},
}

@inproceedings{qiao_energy_2023-1,
	address = {Changchun, China},
	title = {Energy {Efficiency} {Optimization} {Algorithm} {Based} on {CNN}-{SPP}-{AM} for {D2D} {Communication}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-1667-4},
	url = {https://ieeexplore.ieee.org/document/10435507/},
	doi = {10.1109/EIECS59936.2023.10435507},
	abstract = {The underlying multiplexed spectrum in D2D communications has an impact on spectral efficiency and multiplexing can introduce serious interference in communication systems. Many communication devices are subject to limited power supplies. Therefore, Therefore, we need to address the issue of resource allocation in D2D communication and maximise energy efficiency. Therefore, this paper proposes a CNN-SPP-AM based optimisation method for energy efficiency of D2D communication, which firstly pools the pooling layer of the convolutional neural network with spatial pyramids and uses a zero padding strategy in the output layer to remove the input size limitation. Then, the attention mechanism is introduced into the optimised convolutional neural network to improve the learning ability of individual features.The simulation results show that the proposed CNN-SPP-AM outperforms other neural networks in terms of system energy efficiency and system sum rate.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2023 3rd {International} {Conference} on {Electronic} {Information} {Engineering} and {Computer} {Science} ({EIECS})},
	publisher = {IEEE},
	author = {Qiao, Shixia and Zheng, Kaijin and Zhou, Jialu and Qu, Zitao},
	month = sep,
	year = {2023},
	pages = {289--292},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\83KJSAI2\\Qiao et al. - 2023 - Energy Efficiency Optimization Algorithm Based on CNN-SPP-AM for D2D Communication.pdf:application/pdf},
}

@inproceedings{gorvadiya_energy_2025-1,
	address = {Gandhinagar, India},
	title = {Energy {Efficient} {Pruning} and {Quantization} {Methods} for {Deep} {Learning} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-2054-0},
	url = {https://ieeexplore.ieee.org/document/10932458/},
	doi = {10.1109/SETCOM64758.2025.10932458},
	abstract = {As deep learning models are increasingly being deployed on resource-constrained edge devices, the need to develop techniques to make the model more energy efficient without sacrificing its performance becomes crucial. In this study we explored the three most prominent approaches, such as bit quantization, model pruning, and adaptive switching techniques to address the computational and memory overheads of deep neural networks. Bit quantization reduces the weight and activation precisions, effectively lowering the hardware requirements for both storage and computation. Again the model pruning eliminates redundant parameters; thus, reduces the model complexity, and boost the inference time. Along with all these methods the adaptive switching techniques allow the model’s parameters or structure to switch at run-time so that trade-off between accuracy and energy consumption is dynamically optimized. In this paper we explored the advantages and limitations of such techniques for edge devices through a comprehensive study of recent advancements and methodologies. Point out that the open challenge on how to balance model performance with energy efficiency and future directions for the optimization of deep learning models for edge computing environments.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2025 {International} {Conference} on {Sustainable} {Energy} {Technologies} and {Computational} {Intelligence} ({SETCOM})},
	publisher = {IEEE},
	author = {Gorvadiya, Jay and Chagela, Ankur and Roy, Mohendra},
	month = feb,
	year = {2025},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\APQP82KK\\Gorvadiya et al. - 2025 - Energy Efficient Pruning and Quantization Methods for Deep Learning Models.pdf:application/pdf},
}

@article{kim_energy-efficient_2020-1,
	title = {Energy-{Efficient} {Acceleration} of {Deep} {Neural} {Networks} on {Realtime}-{Constrained} {Embedded} {Edge} {Devices}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9262933/},
	doi = {10.1109/ACCESS.2020.3038908},
	abstract = {This paper presents a hardware management technique that enables energy-efﬁcient acceleration of deep neural networks (DNNs) on realtime-constrained embedded edge devices. It becomes increasingly common for edge devices to incorporate dedicated hardware accelerators for neural processing. The execution of neural accelerators in general follows a host-device model, where CPUs ofﬂoad neural computations (e.g., matrix and vector calculations) to the accelerators for datapath-optimized executions. Such a serialized execution is simple to implement and manage, but it is wasteful for the resource-limited edge devices to exercise only a single type of processing unit in a discrete execution phase. This paper presents a hardware management technique named NeuroPipe that utilizes heterogeneous processing units in an embedded edge device to accelerate DNNs in energy-efﬁcient manner. In particular, NeuroPipe splits a neural network into groups of consecutive layers and pipelines their executions using different types of processing units. The proposed technique offers several advantages to accelerate DNN inference in the embedded edge device. It enables the embedded processor to operate at lower voltage and frequency to enhance energy efﬁciency while delivering the same performance as uncontrolled baseline executions, or inversely it can dispatch faster inferences at the same energy consumption. Our measurement-driven experiments based on NVIDIA Jetson AGX Xavier with 64 tensor cores and eight-core ARM CPU demonstrate that NeuroPipe reduces energy consumption by 11.4\% on average without performance degradation, or it can achieve 30.5\% greater performance for the same energy consumption.},
	language = {en},
	urldate = {2025-04-01},
	journal = {IEEE Access},
	author = {Kim, Bogil and Lee, Sungjae and Trivedi, Amit Ranjan and Song, William J.},
	year = {2020},
	pages = {216259--216270},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\R92TF79R\\Kim et al. - 2020 - Energy-Efficient Acceleration of Deep Neural Networks on Realtime-Constrained Embedded Edge Devices.pdf:application/pdf},
}

@article{liu_energy-efficient_2024-1,
	title = {Energy-{Efficient} {Computing} {Acceleration} of {Unmanned} {Aerial} {Vehicles} {Based} on a {CPU}/{FPGA}/{NPU} {Heterogeneous} {System}},
	volume = {11},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/10525057/},
	doi = {10.1109/JIOT.2024.3397649},
	abstract = {The time and energy optimization of computationally intensive tasks involving unmanned air vehicles (UAVs) is highly important for increasing the reaction speed of UAVs and for prolonging their lifetime. To achieve the above objective, many studies based on heterogeneous computing have been carried out. Although these studies have achieved good results, limitations remain. First, neural processing units (NPUs) have emerged in recent years. However, insufﬁcient attention has been devoted to CPU/NPU research in academia currently. Second, most popular heterogeneous computing architectures have only one kind of accelerator, e.g., CPU/GPU or CPU/ﬁeld programmable gate array (FPGA). A heterogeneous system with multiple kinds of accelerators, e.g., CPU/FPGA/NPU, has not been investigated in depth. To address the above concerns, we propose a heterogeneous CPU/FPGA/NPU system aimed at realizing energy-efﬁcient computing acceleration for computationally intensive UAV tasks. First, we select several representative computationally intensive UAV tasks and design FPGA and NPU accelerators dedicated to these tasks. Then, we calculate the time and energy costs of these tasks on the FPGA and NPU, respectively, and ﬁnd that different tasks are appropriate for running on different cores. Based on this ﬁnding, we further build a heterogeneous CPU/FPGA/NPU architecture and assign each UAV task to the most appropriate core for execution. In this way, the UAV tasks can be executed more efﬁciently. We conduct experiments by executing all the representative UAV tasks on the CPU, CPU/GPU, CPU/FPGA, CPU/NPU and CPU/FPGA/NPU platforms. The results show that a heterogeneous system with multiple accelerators can achieve better computing performance and higher energy efﬁciency.},
	language = {en},
	number = {16},
	urldate = {2025-04-01},
	journal = {IEEE Internet of Things Journal},
	author = {Liu, Xing and Xu, Wenxing and Wang, Qing and Zhang, Mengya},
	month = aug,
	year = {2024},
	pages = {27126--27138},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\WMLDYS46\\Liu et al. - 2024 - Energy-Efficient Computing Acceleration of Unmanned Aerial Vehicles Based on a CPUFPGANPU Heteroge.pdf:application/pdf},
}

@inproceedings{kim_fpga_2021-1,
	address = {Paris, France},
	title = {{FPGA} {Prototyping} of {Systolic} {Array}-based {Accelerator} for {Low}-{Precision} {Inference} of {Deep} {Neural} {Networks}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6956-2},
	url = {https://ieeexplore.ieee.org/document/9806200/},
	doi = {10.1109/RSP53691.2021.9806200},
	abstract = {In this study, we aim to design an energy-efﬁcient computation system for deep neural networks on edge devices. To maximize energy efﬁciency, we design a novel hardware accelerator that supports low-precision computation and sparsityaware structured zero-skipping on top of the well-known systolicarray structure. In addition, we introduce a full-stack software platform, including a model optimizer, instruction compiler, and host interface, to translate the pre-trained PyTorch model to the proposed accelerator and orchestrate it automatically. We validate the entire system by prototyping the accelerator on the Xilinx Alveo U250 FPGA board and demonstrating the inference of the 4-bit ResNet-50 model through the software stack. According to our experiment, our platform shows 317 GOPS inference speed and 51.96 GOPS/W energy efﬁciency for ResNet-50 on Xilinx Alveo U250 FPGA at 108 MHz, which is comparable to the advanced commercial acceleration system in terms of energy efﬁciency.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2021 {IEEE} {International} {Workshop} on {Rapid} {System} {Prototyping} ({RSP})},
	publisher = {IEEE},
	author = {Kim, Soobeom and Cho, Seunghwan and Park, Eunhyeok and Yoo, Sungjoo},
	month = oct,
	year = {2021},
	pages = {1--7},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\F7LKBIQR\\Kim et al. - 2021 - FPGA Prototyping of Systolic Array-based Accelerator for Low-Precision Inference of Deep Neural Netw.pdf:application/pdf},
}

@inproceedings{oh_investigation_2017-1,
	address = {Kuta Bali},
	title = {Investigation on performance and energy efficiency of {CNN}-based object detection on embedded device},
	isbn = {978-1-5386-0600-1},
	url = {http://ieeexplore.ieee.org/document/8320657/},
	doi = {10.1109/CAIPT.2017.8320657},
	abstract = {The use of a Convolutional Neural Network based method for object detection increases the accuracy that surpasses human visual system. Because it requires considerable computational capability, its use in embedded devices that place constraints in terms of power consumption as well as computational capability has thus far been limited. However, with the recent development of GPU for use in embedded devices and open-source software library for machine learning, it has become viable to utilize CNN in an energy-efficient embedded computing environment. In this study, CPU and GPU performance and energy efficiency of CNN-based object detection inference on an embedded platform is investigated through comparison with a traditional PC-based platform. Two publicly available hardware platforms are empirically evaluated; in one of them—NVIDIA Jetson TX-1—the results demonstrate image processing performance of 65\% of that of the PC, while the embedded device consumes 2.6\% of power consumed by the PC.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2017 4th {International} {Conference} on {Computer} {Applications} and {Information} {Processing} {Technology} ({CAIPT})},
	publisher = {IEEE},
	author = {Oh, Sangyoon and Kim, Minsub and Kim, Donghoon and Jeong, Minjoong and Lee, Minsu},
	month = aug,
	year = {2017},
	pages = {1--4},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\8R4636GB\\Oh et al. - 2017 - Investigation on performance and energy efficiency of CNN-based object detection on embedded device.pdf:application/pdf},
}

@inproceedings{liu_leveraging_2021,
	address = {Dresden, Germany},
	title = {Leveraging {Fine}-grained {Structured} {Sparsity} for {CNN} {Inference} on {Systolic} {Array} {Architectures}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-3759-2},
	url = {https://ieeexplore.ieee.org/document/9556391/},
	doi = {10.1109/FPL53798.2021.00060},
	abstract = {The high computational complexity of convolutional neural networks (CNNs) has motivated many studies of accelerating CNN inference on ﬁeld-programmable gate arrays (FPGAs). Among these, designs that feature systolic arrays can effectively leverage the parallelism in CNNs while acheiving good placement and routing quality. Weight sparsity — the presence of zeros in CNN weights — can further reduce the number of necessary multiply-accumulate (MAC) operations in CNNs, but has yet resulted in performance gain on systolic arrays. In this work, we propose a novel ﬁne-grained structured weight sparsity pattern, showcase a processing element (PE) design that leverages this sparsity pattern, and develop a systolic array CNN inference accelerator that targets an Intel Arria 10 GX1150 FPGA. When evaluated on ResNet-50 and VGG-16 that are trained and pruned on the ImageNet dataset, our accelerator achieves 2.26 TOPs/s and 1.21 TOPs/s, respectively, on the MAC operations, while keeping the top-1 accuracy degradation within 5\%. These results translate to 2.86× and 1.75× speed-up compared to a dense systolic array baseline.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2021 31st {International} {Conference} on {Field}-{Programmable} {Logic} and {Applications} ({FPL})},
	publisher = {IEEE},
	author = {Liu, Linqiao and Brown, Stephen},
	month = aug,
	year = {2021},
	pages = {301--305},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\MB9RDXQI\\Liu and Brown - 2021 - Leveraging Fine-grained Structured Sparsity for CNN Inference on Systolic Array Architectures.pdf:application/pdf},
}

@inproceedings{ks_modified_2022-1,
	address = {Malang, Indonesia},
	title = {Modified {CNN} to {Maximize} {Energy} {Efficiency} in {D2D} {Underlying} with {Multi}-{Cell} {Cellular} {Network}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-9742-8},
	url = {https://ieeexplore.ieee.org/document/9865637/},
	doi = {10.1109/CyberneticsCom55287.2022.9865637},
	abstract = {The usage of Device-to-Device (D2D) underlaying to reuse spectrum has a substantial inﬂuence on spectrum efﬁciency. On the other side, interference issues arise as a result of frequency reused by D2D users. Furthermore, wearable devices or communication devices have limited power sources, such as batteries. As a result, the fundamental problem formulation that must be solved is power allocation, with the goal function being to maximize the energy efﬁciency of the system. In order to provide optimum power allocation, conventional methods such as Convex Approximation (CA)based algorithm need to run multiple iterations to solve the non-convex problem formulation. Therefore, Convolution Neural Network (CNN) as part of Deep Learning (DL) is utilized to approach (CA)-based algorithm for generating power allocation policies to maximize the systems energy efﬁciency. However, the conventional method of CNN has limitations in accepting arbitrary input size. Accordingly, to the limitation of CNN, this research proposed the combination of CNN with Spatial Pyramid Pooling (SPP) to overcome the limitation on the input size of conventional CNN. Speciﬁcally, the inputs of the model are the user’s channel state information, and its outputs are power control policies. The simulation results show that both CNN-SPP and CNN can achieve similar performance to the traditional method up to 95\% accuracy. Furthermore, the combination of CNN and SPP can overcome the limitation on the input size of the conventional CNN method, reducing the number of models that must be trained to just one and applying it to all scenarios regardless of the number of CUEs D2D pairs.},
	language = {en},
	urldate = {2025-04-01},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cybernetics} and {Computational} {Intelligence} ({CyberneticsCom})},
	publisher = {IEEE},
	author = {K.S, Bayu Setho and Fahmi, Arfianto and Adriansyah, Nachwan Mufti and W. Prabowo, V. S.},
	month = jun,
	year = {2022},
	pages = {359--364},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\TI6XQTQ5\\K.S et al. - 2022 - Modified CNN to Maximize Energy Efficiency in D2D Underlying with Multi-Cell Cellular Network.pdf:application/pdf},
}
