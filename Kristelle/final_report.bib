
@inproceedings{gorvadiya_energy_2025,
	address = {Gandhinagar, India},
	title = {Energy {Efficient} {Pruning} and {Quantization} {Methods} for {Deep} {Learning} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-2054-0},
	url = {https://ieeexplore.ieee.org/document/10932458/},
	doi = {10.1109/SETCOM64758.2025.10932458},
	abstract = {As deep learning models are increasingly being deployed on resource-constrained edge devices, the need to develop techniques to make the model more energy efficient without sacrificing its performance becomes crucial. In this study we explored the three most prominent approaches, such as bit quantization, model pruning, and adaptive switching techniques to address the computational and memory overheads of deep neural networks. Bit quantization reduces the weight and activation precisions, effectively lowering the hardware requirements for both storage and computation. Again the model pruning eliminates redundant parameters; thus, reduces the model complexity, and boost the inference time. Along with all these methods the adaptive switching techniques allow the model’s parameters or structure to switch at run-time so that trade-off between accuracy and energy consumption is dynamically optimized. In this paper we explored the advantages and limitations of such techniques for edge devices through a comprehensive study of recent advancements and methodologies. Point out that the open challenge on how to balance model performance with energy efficiency and future directions for the optimization of deep learning models for edge computing environments.},
	language = {en},
	urldate = {2025-03-28},
	booktitle = {2025 {International} {Conference} on {Sustainable} {Energy} {Technologies} and {Computational} {Intelligence} ({SETCOM})},
	publisher = {IEEE},
	author = {Gorvadiya, Jay and Chagela, Ankur and Roy, Mohendra},
	month = feb,
	year = {2025},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\H6XN8PYV\\Gorvadiya et al. - 2025 - Energy Efficient Pruning and Quantization Methods for Deep Learning Models.pdf:application/pdf},
}

@inproceedings{kim_fpga_2021,
	address = {Paris, France},
	title = {{FPGA} {Prototyping} of {Systolic} {Array}-based {Accelerator} for {Low}-{Precision} {Inference} of {Deep} {Neural} {Networks}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6956-2},
	url = {https://ieeexplore.ieee.org/document/9806200/},
	doi = {10.1109/RSP53691.2021.9806200},
	abstract = {In this study, we aim to design an energy-efﬁcient computation system for deep neural networks on edge devices. To maximize energy efﬁciency, we design a novel hardware accelerator that supports low-precision computation and sparsityaware structured zero-skipping on top of the well-known systolicarray structure. In addition, we introduce a full-stack software platform, including a model optimizer, instruction compiler, and host interface, to translate the pre-trained PyTorch model to the proposed accelerator and orchestrate it automatically. We validate the entire system by prototyping the accelerator on the Xilinx Alveo U250 FPGA board and demonstrating the inference of the 4-bit ResNet-50 model through the software stack. According to our experiment, our platform shows 317 GOPS inference speed and 51.96 GOPS/W energy efﬁciency for ResNet-50 on Xilinx Alveo U250 FPGA at 108 MHz, which is comparable to the advanced commercial acceleration system in terms of energy efﬁciency.},
	language = {en},
	urldate = {2025-03-30},
	booktitle = {2021 {IEEE} {International} {Workshop} on {Rapid} {System} {Prototyping} ({RSP})},
	publisher = {IEEE},
	author = {Kim, Soobeom and Cho, Seunghwan and Park, Eunhyeok and Yoo, Sungjoo},
	month = oct,
	year = {2021},
	pages = {1--7},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\CMDGP9N6\\Kim et al. - 2021 - FPGA Prototyping of Systolic Array-based Accelerator for Low-Precision Inference of Deep Neural Netw.pdf:application/pdf},
}

@article{liu_energy-efficient_2024,
	title = {Energy-{Efficient} {Computing} {Acceleration} of {Unmanned} {Aerial} {Vehicles} {Based} on a {CPU}/{FPGA}/{NPU} {Heterogeneous} {System}},
	volume = {11},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2327-4662, 2372-2541},
	url = {https://ieeexplore.ieee.org/document/10525057/},
	doi = {10.1109/JIOT.2024.3397649},
	abstract = {The time and energy optimization of computationally intensive tasks involving unmanned air vehicles (UAVs) is highly important for increasing the reaction speed of UAVs and for prolonging their lifetime. To achieve the above objective, many studies based on heterogeneous computing have been carried out. Although these studies have achieved good results, limitations remain. First, neural processing units (NPUs) have emerged in recent years. However, insufﬁcient attention has been devoted to CPU/NPU research in academia currently. Second, most popular heterogeneous computing architectures have only one kind of accelerator, e.g., CPU/GPU or CPU/ﬁeld programmable gate array (FPGA). A heterogeneous system with multiple kinds of accelerators, e.g., CPU/FPGA/NPU, has not been investigated in depth. To address the above concerns, we propose a heterogeneous CPU/FPGA/NPU system aimed at realizing energy-efﬁcient computing acceleration for computationally intensive UAV tasks. First, we select several representative computationally intensive UAV tasks and design FPGA and NPU accelerators dedicated to these tasks. Then, we calculate the time and energy costs of these tasks on the FPGA and NPU, respectively, and ﬁnd that different tasks are appropriate for running on different cores. Based on this ﬁnding, we further build a heterogeneous CPU/FPGA/NPU architecture and assign each UAV task to the most appropriate core for execution. In this way, the UAV tasks can be executed more efﬁciently. We conduct experiments by executing all the representative UAV tasks on the CPU, CPU/GPU, CPU/FPGA, CPU/NPU and CPU/FPGA/NPU platforms. The results show that a heterogeneous system with multiple accelerators can achieve better computing performance and higher energy efﬁciency.},
	language = {en},
	number = {16},
	urldate = {2025-03-31},
	journal = {IEEE Internet of Things Journal},
	author = {Liu, Xing and Xu, Wenxing and Wang, Qing and Zhang, Mengya},
	month = aug,
	year = {2024},
	pages = {27126--27138},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\TCDCGNCT\\Liu et al. - 2024 - Energy-Efficient Computing Acceleration of Unmanned Aerial Vehicles Based on a CPUFPGANPU Heteroge.pdf:application/pdf},
}

@article{manor_custom_2022,
	title = {Custom {Hardware} {Inference} {Accelerator} for {TensorFlow} {Lite} for {Microcontrollers}},
	volume = {10},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9825651/},
	doi = {10.1109/ACCESS.2022.3189776},
	abstract = {In recent years, the need for the efﬁcient deployment of Neural Networks (NN) on edge devices has been steadily increasing. However, the high computational demand required for Machine Learning (ML) inference on tiny microcontroller-based IoT devices avoids a direct software deployment on such resource-constrained edge devices. Therefore, various custom and application-speciﬁc NN hardware accelerators have been proposed to enable real-time Machine Learning (ML) inference on low-power and resource-limited edge devices. Efﬁcient mapping of the computational load onto hardware and software resources is a key challenge for performance improvement while keeping low power and a low area footprint. High performance and yet low power embedded processors may be attained via the usage of hardware acceleration. This paper presents an efﬁcient hardware-software framework to accelerate machine learning inference on edge devices using a modiﬁed TensorFlow Lite for Microcontroller (TFLM) model running on a Microcontroller (MCU) and a dedicated Neural Processing Unit (NPU) custom hardware accelerator, referred to as MCU-NPU. The proposed framework supports weight compression of pruned quantized NN models and exploits the pruned model sparsity to reduce computational complexity further. The proposed methodology has been evaluated by employing the MCU-NPU acceleration for various TFLM-based NN architectures using the common MLPerf Tiny benchmark. Experimental results demonstrate a signiﬁcant speedup of up to 724x compared to a pure software implementation. For example, the resulting runtime for the CIFAR-10 classiﬁcation is reduced from about 20 sec to only 37 ms using the proposed hardware acceleration. Moreover, the proposed hardware accelerator outperforms all the reference models optimized for edge devices in terms of inference runtime.},
	language = {en},
	urldate = {2025-03-31},
	journal = {IEEE Access},
	author = {Manor, Erez and Greenberg, Shlomo},
	year = {2022},
	pages = {73484--73493},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\CLBGJ4QU\\Manor and Greenberg - 2022 - Custom Hardware Inference Accelerator for TensorFlow Lite for Microcontrollers.pdf:application/pdf},
}

@inproceedings{qiao_energy_2023,
	address = {Changchun, China},
	title = {Energy {Efficiency} {Optimization} {Algorithm} {Based} on {CNN}-{SPP}-{AM} for {D2D} {Communication}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-1667-4},
	url = {https://ieeexplore.ieee.org/document/10435507/},
	doi = {10.1109/EIECS59936.2023.10435507},
	abstract = {The underlying multiplexed spectrum in D2D communications has an impact on spectral efficiency and multiplexing can introduce serious interference in communication systems. Many communication devices are subject to limited power supplies. Therefore, Therefore, we need to address the issue of resource allocation in D2D communication and maximise energy efficiency. Therefore, this paper proposes a CNN-SPP-AM based optimisation method for energy efficiency of D2D communication, which firstly pools the pooling layer of the convolutional neural network with spatial pyramids and uses a zero padding strategy in the output layer to remove the input size limitation. Then, the attention mechanism is introduced into the optimised convolutional neural network to improve the learning ability of individual features.The simulation results show that the proposed CNN-SPP-AM outperforms other neural networks in terms of system energy efficiency and system sum rate.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2023 3rd {International} {Conference} on {Electronic} {Information} {Engineering} and {Computer} {Science} ({EIECS})},
	publisher = {IEEE},
	author = {Qiao, Shixia and Zheng, Kaijin and Zhou, Jialu and Qu, Zitao},
	month = sep,
	year = {2023},
	pages = {289--292},
	annote = {No citations on mindmap - did not seem relevant to the project
},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\G68GLCKF\\Qiao et al. - 2023 - Energy Efficiency Optimization Algorithm Based on CNN-SPP-AM for D2D Communication.pdf:application/pdf},
}

@inproceedings{ks_modified_2022,
	address = {Malang, Indonesia},
	title = {Modified {CNN} to {Maximize} {Energy} {Efficiency} in {D2D} {Underlying} with {Multi}-{Cell} {Cellular} {Network}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-9742-8},
	url = {https://ieeexplore.ieee.org/document/9865637/},
	doi = {10.1109/CyberneticsCom55287.2022.9865637},
	abstract = {The usage of Device-to-Device (D2D) underlaying to reuse spectrum has a substantial inﬂuence on spectrum efﬁciency. On the other side, interference issues arise as a result of frequency reused by D2D users. Furthermore, wearable devices or communication devices have limited power sources, such as batteries. As a result, the fundamental problem formulation that must be solved is power allocation, with the goal function being to maximize the energy efﬁciency of the system. In order to provide optimum power allocation, conventional methods such as Convex Approximation (CA)based algorithm need to run multiple iterations to solve the non-convex problem formulation. Therefore, Convolution Neural Network (CNN) as part of Deep Learning (DL) is utilized to approach (CA)-based algorithm for generating power allocation policies to maximize the systems energy efﬁciency. However, the conventional method of CNN has limitations in accepting arbitrary input size. Accordingly, to the limitation of CNN, this research proposed the combination of CNN with Spatial Pyramid Pooling (SPP) to overcome the limitation on the input size of conventional CNN. Speciﬁcally, the inputs of the model are the user’s channel state information, and its outputs are power control policies. The simulation results show that both CNN-SPP and CNN can achieve similar performance to the traditional method up to 95\% accuracy. Furthermore, the combination of CNN and SPP can overcome the limitation on the input size of the conventional CNN method, reducing the number of models that must be trained to just one and applying it to all scenarios regardless of the number of CUEs D2D pairs.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2022 {IEEE} {International} {Conference} on {Cybernetics} and {Computational} {Intelligence} ({CyberneticsCom})},
	publisher = {IEEE},
	author = {K.S, Bayu Setho and Fahmi, Arfianto and Adriansyah, Nachwan Mufti and W. Prabowo, V. S.},
	month = jun,
	year = {2022},
	pages = {359--364},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\3FCMFQDA\\K.S et al. - 2022 - Modified CNN to Maximize Energy Efficiency in D2D Underlying with Multi-Cell Cellular Network.pdf:application/pdf},
}

@inproceedings{oh_investigation_2017,
	address = {Kuta Bali},
	title = {Investigation on performance and energy efficiency of {CNN}-based object detection on embedded device},
	isbn = {978-1-5386-0600-1},
	url = {http://ieeexplore.ieee.org/document/8320657/},
	doi = {10.1109/CAIPT.2017.8320657},
	abstract = {The use of a Convolutional Neural Network based method for object detection increases the accuracy that surpasses human visual system. Because it requires considerable computational capability, its use in embedded devices that place constraints in terms of power consumption as well as computational capability has thus far been limited. However, with the recent development of GPU for use in embedded devices and open-source software library for machine learning, it has become viable to utilize CNN in an energy-efficient embedded computing environment. In this study, CPU and GPU performance and energy efficiency of CNN-based object detection inference on an embedded platform is investigated through comparison with a traditional PC-based platform. Two publicly available hardware platforms are empirically evaluated; in one of them—NVIDIA Jetson TX-1—the results demonstrate image processing performance of 65\% of that of the PC, while the embedded device consumes 2.6\% of power consumed by the PC.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2017 4th {International} {Conference} on {Computer} {Applications} and {Information} {Processing} {Technology} ({CAIPT})},
	publisher = {IEEE},
	author = {Oh, Sangyoon and Kim, Minsub and Kim, Donghoon and Jeong, Minjoong and Lee, Minsu},
	month = aug,
	year = {2017},
	pages = {1--4},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\K5V7H78L\\Oh et al. - 2017 - Investigation on performance and energy efficiency of CNN-based object detection on embedded device.pdf:application/pdf},
}

@inproceedings{guha_secure_2024,
	address = {Knoxville, TN, USA},
	title = {Secure {Energy}-{Efficient} {Implementation} of {CNN} on {FPGAs} for {Accuracy} {Dependent} {Real} {Time} {Task} {Processing}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-5411-9},
	url = {https://ieeexplore.ieee.org/document/10682707/},
	doi = {10.1109/ISVLSI61997.2024.00012},
	abstract = {A key objective of the fourth industrial revolution or Industry 4.0 is to use processing resources that are reconﬁgurable and ﬂexible, having additional capability to run smart real time intelligent applications in short time. For this, designers deploy reconﬁgurable hardware or ﬁeld programmable gate arrays (FPGAs) in several critical infrastructures, along with cloud and edge platforms. Among the commonly used artiﬁcial neural networks, convolutional neural networks (CNNs) is one of the most widely used for various smart applications. Existing strategies of CNN implementation on FPGAs incur signiﬁcant resources and power, and hence, are not energy efﬁcient. These are even prone to side channel attacks. Moreover, they are associated with prunning and hence, do not generate accurate results, which are important for some real time applications. In our proposed methodology, we pre-compute various operations, mainly operations associated with secret information, which in the present case are weights and bias of the CNN for a particular application and store in available embedded memory blocks (EMBs) of an FPGA. On demand, these pre-computed results are accessed for real time operations. Via this methodology, we eradicate the side channel attacks that steals the weights and biases, obtain low resource utilization, low latency, low power and better energy efﬁciency, without any loss of accuracy. Such a mechanism is particularly suitable for high accuracy real time smart intelligent applications.},
	language = {en},
	urldate = {2025-03-31},
	booktitle = {2024 {IEEE} {Computer} {Society} {Annual} {Symposium} on {VLSI} ({ISVLSI})},
	publisher = {IEEE},
	author = {Guha, Krishnendu and Chakrabarti, Amlan},
	month = jul,
	year = {2024},
	pages = {1--6},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\T5N9FG58\\Guha and Chakrabarti - 2024 - Secure Energy-Efficient Implementation of CNN on FPGAs for Accuracy Dependent Real Time Task Process.pdf:application/pdf},
}

@inproceedings{noauthor_no_2017,
	address = {Guangzhou, China},
	title = {[{No} title found]},
	isbn = {978-1-5386-3790-6},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Parallel} and {Distributed} {Processing} with {Applications} and 2017 {IEEE} {International} {Conference} on {Ubiquitous} {Computing} and {Communications} ({ISPA}/{IUCC})},
	publisher = {IEEE},
	month = dec,
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\MA9PWBQ4\\2017 - [No title found].pdf:application/pdf},
}

@article{kim_energy-efficient_2020,
	title = {Energy-{Efficient} {Acceleration} of {Deep} {Neural} {Networks} on {Realtime}-{Constrained} {Embedded} {Edge} {Devices}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9262933/},
	doi = {10.1109/ACCESS.2020.3038908},
	abstract = {This paper presents a hardware management technique that enables energy-efﬁcient acceleration of deep neural networks (DNNs) on realtime-constrained embedded edge devices. It becomes increasingly common for edge devices to incorporate dedicated hardware accelerators for neural processing. The execution of neural accelerators in general follows a host-device model, where CPUs ofﬂoad neural computations (e.g., matrix and vector calculations) to the accelerators for datapath-optimized executions. Such a serialized execution is simple to implement and manage, but it is wasteful for the resource-limited edge devices to exercise only a single type of processing unit in a discrete execution phase. This paper presents a hardware management technique named NeuroPipe that utilizes heterogeneous processing units in an embedded edge device to accelerate DNNs in energy-efﬁcient manner. In particular, NeuroPipe splits a neural network into groups of consecutive layers and pipelines their executions using different types of processing units. The proposed technique offers several advantages to accelerate DNN inference in the embedded edge device. It enables the embedded processor to operate at lower voltage and frequency to enhance energy efﬁciency while delivering the same performance as uncontrolled baseline executions, or inversely it can dispatch faster inferences at the same energy consumption. Our measurement-driven experiments based on NVIDIA Jetson AGX Xavier with 64 tensor cores and eight-core ARM CPU demonstrate that NeuroPipe reduces energy consumption by 11.4\% on average without performance degradation, or it can achieve 30.5\% greater performance for the same energy consumption.},
	language = {en},
	urldate = {2025-03-31},
	journal = {IEEE Access},
	author = {Kim, Bogil and Lee, Sungjae and Trivedi, Amit Ranjan and Song, William J.},
	year = {2020},
	pages = {216259--216270},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\XMUPQR64\\Kim et al. - 2020 - Energy-Efficient Acceleration of Deep Neural Networks on Realtime-Constrained Embedded Edge Devices.pdf:application/pdf},
}

@article{que_remarn_2023,
	title = {Remarn: {A} {Reconfigurable} {Multi}-threaded {Multi}-core {Accelerator} for {Recurrent} {Neural} {Networks}},
	volume = {16},
	issn = {1936-7406, 1936-7414},
	shorttitle = {Remarn},
	url = {https://dl.acm.org/doi/10.1145/3534969},
	doi = {10.1145/3534969},
	abstract = {This work introduces Remarn, a reconfigurable multi-threaded multi-core accelerator supporting both spatial and temporal co-execution of Recurrent Neural Network (RNN) inferences. It increases processing capabilities and quality of service of cloud-based neural processing units (NPUs) by improving their hardware utilization and by reducing design latency, with two innovations. First, a custom coarse-grained multi-threaded RNN/Long Short-Term Memory (LSTM) hardware architecture, switching tasks among threads when RNN computational engines meet data hazards. Second, the partitioning of this hardware architecture into multiple full-fledged sub-accelerator cores, enabling spatially co-execution of multiple RNN/LSTM inferences. These innovations improve the exploitation of the available parallelism to increase runtime hardware utilization and boost design throughput. Evaluation results show that a dual-threaded quad-core Remarn NPU achieves 2.91 times higher performance while only occupying 5.0\% more area than a single-threaded one on a Stratix 10 FPGA. When compared with a Tesla V100 GPU implementation, our design achieves 6.5 times better performance and 15.6 times higher power efficiency, showing that our approach contributes to high performance and energy-efficient FPGA-based multi-RNN inference designs for datacenters.},
	language = {en},
	number = {1},
	urldate = {2025-03-31},
	journal = {ACM Transactions on Reconfigurable Technology and Systems},
	author = {Que, Zhiqiang and Nakahara, Hiroki and Fan, Hongxiang and Li, He and Meng, Jiuxi and Tsoi, Kuen Hung and Niu, Xinyu and Nurvitadhi, Eriko and Luk, Wayne},
	month = mar,
	year = {2023},
	pages = {1--26},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\YF486GAK\\Que et al. - 2023 - Remarn A Reconfigurable Multi-threaded Multi-core Accelerator for Recurrent Neural Networks.pdf:application/pdf},
}

@article{kim_cnn_2023,
	title = {A {CNN} {Inference} {Accelerator} on {FPGA} {With} {Compression} and {Layer}-{Chaining} {Techniques} for {Style} {Transfer} {Applications}},
	volume = {70},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	url = {https://ieeexplore.ieee.org/document/10013941/},
	doi = {10.1109/TCSI.2023.3234640},
	abstract = {Recently, convolutional neural networks (CNNs) have actively been applied to computer vision applications such as style transfer that changes the style of a content image into that of a style image. As the style transfer CNNs are based on encoder-decoder network architecture and should deal with high-resolution images that become mainstream these days, the computational complexity and the feature map size are very large, preventing the CNNs from being implemented on an FPGA. This paper proposes a CNN inference accelerator for the style transfer applications, which employs network compression and layer-chaining techniques. The network compression technique is to make a style transfer CNN have low computational complexity and a small amount of parameters, and an efﬁcient data compression method is proposed to reduce the feature map size. In addition, the layer-chaining technique is proposed to reduce the off-chip memory trafﬁc and thus to increase the throughput at the cost of small hardware resources. In the proposed hardware architecture, a neural processing unit is designed by taking into account the proposed data compression and layer-chaining techniques. A prototype accelerator implemented on a FPGA board achieves a throughput comparable to the state-of-the-art accelerators developed for encoder-decoder CNNs.},
	language = {en},
	number = {4},
	urldate = {2025-04-01},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Kim, Suchang and Jang, Boseon and Lee, Jaeyoung and Bae, Hyungjoon and Jang, Hyejung and Park, In-Cheol},
	month = apr,
	year = {2023},
	pages = {1591--1604},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\V887VXBX\\Kim et al. - 2023 - A CNN Inference Accelerator on FPGA With Compression and Layer-Chaining Techniques for Style Transfe.pdf:application/pdf},
}

@article{kim_energy-efficient_2020-1,
	title = {Energy-{Efficient} {Acceleration} of {Deep} {Neural} {Networks} on {Realtime}-{Constrained} {Embedded} {Edge} {Devices}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9262933/},
	doi = {10.1109/ACCESS.2020.3038908},
	abstract = {This paper presents a hardware management technique that enables energy-efﬁcient acceleration of deep neural networks (DNNs) on realtime-constrained embedded edge devices. It becomes increasingly common for edge devices to incorporate dedicated hardware accelerators for neural processing. The execution of neural accelerators in general follows a host-device model, where CPUs ofﬂoad neural computations (e.g., matrix and vector calculations) to the accelerators for datapath-optimized executions. Such a serialized execution is simple to implement and manage, but it is wasteful for the resource-limited edge devices to exercise only a single type of processing unit in a discrete execution phase. This paper presents a hardware management technique named NeuroPipe that utilizes heterogeneous processing units in an embedded edge device to accelerate DNNs in energy-efﬁcient manner. In particular, NeuroPipe splits a neural network into groups of consecutive layers and pipelines their executions using different types of processing units. The proposed technique offers several advantages to accelerate DNN inference in the embedded edge device. It enables the embedded processor to operate at lower voltage and frequency to enhance energy efﬁciency while delivering the same performance as uncontrolled baseline executions, or inversely it can dispatch faster inferences at the same energy consumption. Our measurement-driven experiments based on NVIDIA Jetson AGX Xavier with 64 tensor cores and eight-core ARM CPU demonstrate that NeuroPipe reduces energy consumption by 11.4\% on average without performance degradation, or it can achieve 30.5\% greater performance for the same energy consumption.},
	language = {en},
	urldate = {2025-04-02},
	journal = {IEEE Access},
	author = {Kim, Bogil and Lee, Sungjae and Trivedi, Amit Ranjan and Song, William J.},
	year = {2020},
	pages = {216259--216270},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\5IMMX9DD\\Kim et al. - 2020 - Energy-Efficient Acceleration of Deep Neural Networks on Realtime-Constrained Embedded Edge Devices.pdf:application/pdf},
}

@misc{noauthor_parallel_nodate,
	title = {Parallel processing – systolic arrays {\textbar} {GeeksforGeeks}},
	url = {https://www.geeksforgeeks.org/parallel-processing-systolic-arrays/},
	urldate = {2025-04-02},
	file = {Parallel processing – systolic arrays | GeeksforGeeks:C\:\\Users\\iamkr\\Zotero\\storage\\J7PQHPKG\\parallel-processing-systolic-arrays.html:text/html},
}

@inproceedings{ito_power-efficient_2016,
	address = {Yokohama, Japan},
	title = {A power-efficient {FPGA} accelerator: {Systolic} array with cache-coherent interface for pair-{HMM} algorithm},
	isbn = {978-1-5090-1386-9},
	shorttitle = {A power-efficient {FPGA} accelerator},
	url = {http://ieeexplore.ieee.org/document/7503681/},
	doi = {10.1109/CoolChips.2016.7503681},
	abstract = {A systolic array is known as a parallel hardware architecture applicable to a wide range of applications. Naive implementations, however, can lead to inefficient resource usage and low power performance. In this paper, we discuss two techniques for improving the hardware resource usage: flexible multi-threading and dummy data padding. The design was implemented to accelerate a pair-HMM algorithm on an FPGA with the IBM POWER8 CAPI (Coherent Accelerator Processor Interface) feature. The CAPI feature simplifies the software design for driving the FPGA accelerator. Our experimental result indicates that the implemented FPGA accelerator executing the pair-HMM algorithm achieves 33x higher power performance than a POWER8 processor chip executing the same algorithm.},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {2016 {IEEE} {Symposium} in {Low}-{Power} and {High}-{Speed} {Chips} ({COOL} {CHIPS} {XIX})},
	publisher = {IEEE},
	author = {Ito, Megumi and Ohara, Moriyoshi},
	month = apr,
	year = {2016},
	pages = {1--3},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\AQERJATJ\\Ito and Ohara - 2016 - A power-efficient FPGA accelerator Systolic array with cache-coherent interface for pair-HMM algori.pdf:application/pdf},
}

@inproceedings{he_sparse-tpu_2020,
	address = {Barcelona Spain},
	title = {Sparse-{TPU}: adapting systolic arrays for sparse matrices},
	isbn = {978-1-4503-7983-0},
	shorttitle = {Sparse-{TPU}},
	url = {https://dl.acm.org/doi/10.1145/3392717.3392751},
	doi = {10.1145/3392717.3392751},
	abstract = {While systolic arrays are widely used for dense-matrix operations, they are seldom used for sparse-matrix operations. In this paper, we show how a systolic array of Multiply-and-Accumulate (MAC) units, similar to Google’s Tensor Processing Unit (TPU), can be adapted to efficiently handle sparse matrices. TPU-like accelerators are built upon a 2D array of MAC units and have demonstrated high throughput and efficiency for dense matrix multiplication, which is a key kernel in machine learning algorithms and is the target of the TPU. In this work, we employ a co-designed approach of first developing a packing technique to condense a sparse matrix and then propose a systolic array based system, Sparse-TPU, abbreviated to STPU, to accommodate the matrix computations for the packed denser matrix counterparts. To demonstrate the efficacy of our co-designed approach, we evaluate sparse matrix-vector multiplication on a broad set of synthetic and real-world sparse matrices. Experimental results show that STPU delivers 16.08× higher performance while consuming 4.39× and 19.79× lower energy for integer (int8) and floating point (float32) implementations, respectively, over a TPU baseline. Meanwhile, STPU has 12.93\% area overhead and an average of 4.14\% increase in dynamic energy over the TPU baseline for the float32 implementation.},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {Proceedings of the 34th {ACM} {International} {Conference} on {Supercomputing}},
	publisher = {ACM},
	author = {He, Xin and Pal, Subhankar and Amarnath, Aporva and Feng, Siying and Park, Dong-Hyeon and Rovinski, Austin and Ye, Haojie and Chen, Yuhan and Dreslinski, Ronald and Mudge, Trevor},
	month = jun,
	year = {2020},
	pages = {1--12},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\C7QAYQ6G\\He et al. - 2020 - Sparse-TPU adapting systolic arrays for sparse matrices.pdf:application/pdf},
}

@article{choi_enabling_2023,
	title = {Enabling {Fine}-{Grained} {Spatial} {Multitasking} on {Systolic}-{Array} {NPUs} {Using} {Dataflow} {Mirroring}},
	volume = {72},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0018-9340, 1557-9956, 2326-3814},
	url = {https://ieeexplore.ieee.org/document/10198513/},
	doi = {10.1109/TC.2023.3299030},
	abstract = {Neural Processing Units (NPUs) frequently suffer from low hardware utilization as the efﬁciency of their systolic arrays heavily depends on the characteristics of a deep neural network (DNN). Spatial multitasking is a promising solution to overcome the low NPU hardware utilization; however, the state-of-the-art spatial-multitasking NPU architecture achieves sub-optimal performance due to its coarse-grained systolic-array distribution and incurs signiﬁcant implementation costs. In this paper, we propose dataﬂow-mirroring NPU (DM-NPU), a novel spatial-multitasking NPU architecture supporting ﬁne-grained systolic-array distribution. The key idea of DM-NPU is to reverse the dataﬂows of co-located DNNs in horizontal and/or vertical directions. DM-NPU can place allocation boundaries between any adjacent processing elements of a systolic array, both horizontally and vertically. We then propose DM-Perf, an accurate systolic-array NPU performance model, to maximize the spatial-multitasking performance of DM-NPU. Utilizing the existing performance models achieves sub-optimal performance as they cannot accurately capture the resource contention caused by spatial multitasking. DM-Perf, on the other hand, exploits the per-layer performance proﬁles of a DNN to accurately capture the resource contention. Our evaluation using MLPerf DNNs shows that DM-NPU and DM-Perf can greatly improve the performance by up to 35.1\% over the state-of-the-art NPU architecture and performance model.},
	language = {en},
	number = {12},
	urldate = {2025-04-02},
	journal = {IEEE Transactions on Computers},
	author = {Choi, Jinwoo and Ha, Yeonan and Lee, Jounghoo and Lee, Sangsu and Lee, Jinho and Jang, Hanhwi and Kim, Youngsok},
	month = dec,
	year = {2023},
	pages = {3383--3398},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\QT4LJAZS\\Choi et al. - 2023 - Enabling Fine-Grained Spatial Multitasking on Systolic-Array NPUs Using Dataflow Mirroring.pdf:application/pdf},
}

@inproceedings{ghodrati_planaria_2020,
	address = {Athens, Greece},
	title = {Planaria: {Dynamic} {Architecture} {Fission} for {Spatial} {Multi}-{Tenant} {Acceleration} of {Deep} {Neural} {Networks}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-7383-2},
	shorttitle = {Planaria},
	url = {https://ieeexplore.ieee.org/document/9251939/},
	doi = {10.1109/MICRO50266.2020.00062},
	abstract = {Deep Neural Networks (DNNs) have reinvigorated real-world applications that rely on learning patterns of data and are permeating into different industries and markets. Cloud infrastructure and accelerators that offer INFerence-as-a-Service (INFaaS) have become the enabler of this rather quick and invasive shift in the industry. To that end, mostly acceleratorbased INFaaS (Google’s TPU [1], NVIDIA T4 [2], Microsoft Brainwave [3], etc.) has become the backbone of many real-life applications. However, as the demand for such services grows, merely scaling-out the number of accelerators is not economically cost-effective. Although multi-tenancy has propelled datacenter scalability, it has not been a primary factor in designing DNN accelerators due to the arms race for higher speed and efﬁciency. This paper sets out to explore this timely requirement of multitenancy through a new dimension: dynamic architecture ﬁssion. To that end, we deﬁne Planaria1 that can dynamically ﬁssion (break) into multiple smaller yet full-ﬂedged DNN engines at runtime. This microarchitectural capability enables spatially colocating multiple DNN inference services on the same hardware, offering simultaneous multi-tenant DNN acceleration. To realize this dynamic reconﬁgurability, we ﬁrst devise breakable omnidirectional systolic arrays for DNN acceleration that allows omnidirectional ﬂow of data. Second, it uses this capability and a unique organization of on-chip memory, interconnection, and compute resources to enable ﬁssion in systolic array based DNN accelerators. Architecture ﬁssion and its associated ﬂexibility enables an extra degree of freedom for task scheduling, that even allows breaking the accelerator with regard to the server load, DNN topology, and task priority. As such, it can simultaneously co-locate DNNs to enhance utilization, throughput, QoS, and fairness. We compare the proposed design to PREMA [4], a recent effort that offers multi-tenancy by time-multiplexing the DNN accelerator across multiple tasks. We use the same frequency, the same amount of compute and memory resources for both accelerators. The results show signiﬁcant beneﬁts with (soft, medium, hard) QoS requirements, in throughput (7.4×, 7.2×, 12.2×), SLA satisfaction rate (45\%, 15\%, 16\%), and fairness (2.1×, 2.3×, 1.9×).},
	language = {en},
	urldate = {2025-04-02},
	booktitle = {2020 53rd {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture} ({MICRO})},
	publisher = {IEEE},
	author = {Ghodrati, Soroush and Ahn, Byung Hoon and Kyung Kim, Joon and Kinzer, Sean and Yatham, Brahmendra Reddy and Alla, Navateja and Sharma, Hardik and Alian, Mohammad and Ebrahimi, Eiman and Kim, Nam Sung and Young, Cliff and Esmaeilzadeh, Hadi},
	month = oct,
	year = {2020},
	pages = {681--697},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\T8NYB7U3\\Ghodrati et al. - 2020 - Planaria Dynamic Architecture Fission for Spatial Multi-Tenant Acceleration of Deep Neural Networks.pdf:application/pdf},
}

@misc{noauthor_what_nodate,
	title = {What is low-precision computing? - {IBM} {Research}},
	url = {https://research.ibm.com/blog/low-precision-computing},
	urldate = {2025-04-02},
	file = {What is low-precision computing? - IBM Research:C\:\\Users\\iamkr\\Zotero\\storage\\9BEP6QUK\\low-precision-computing.html:text/html},
}

@misc{noauthor_but_nodate,
	title = {But what is a neural network? {\textbar} {Deep} learning chapter 1 - {YouTube}},
	url = {https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown},
	urldate = {2025-04-02},
	file = {But what is a neural network? | Deep learning chapter 1 - YouTube:C\:\\Users\\iamkr\\Zotero\\storage\\K2J8BKDK\\watch.html:text/html},
}

@misc{noauthor_what_nodate-1,
	title = {What are {Convolutional} {Neural} {Networks} ({CNNs})? - {YouTube}},
	url = {https://www.youtube.com/watch?v=QzY57FaENXg&ab_channel=IBMTechnology},
	urldate = {2025-04-02},
	file = {What are Convolutional Neural Networks (CNNs)? - YouTube:C\:\\Users\\iamkr\\Zotero\\storage\\UU2ISD9T\\watch.html:text/html},
}

@article{wang_pl-npu_2022,
	title = {{PL}-{NPU}: {An} {Energy}-{Efficient} {Edge}-{Device} {DNN} {Training} {Processor} {With} {Posit}-{Based} {Logarithm}-{Domain} {Computing}},
	volume = {69},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1549-8328, 1558-0806},
	shorttitle = {{PL}-{NPU}},
	url = {https://ieeexplore.ieee.org/document/9803862/},
	doi = {10.1109/TCSI.2022.3184115},
	abstract = {Edge device deep neural network (DNN) training is practical to improve model adaptivity for unfamiliar datasets while avoiding privacy disclosure and huge communication cost. Nevertheless, apart from feed-forward (FF) as inference, DNN training still requires back-propagation (BP) and weight gradient (WG), introducing power-consuming ﬂoating-point computing requirements, hardware underutilization, and energy bottleneck from excessive memory access. This paper proposes a DNN training processor named PL-NPU to solve the above challenges with three innovations. First, a posit-based logarithmdomain processing element (PE) adapts to various training data requirements with a low bit-width format and reduces energy by transferring complicated arithmetics into simple logarithm domain operation. Second, a reconﬁgurable inter-intra-channelreuse dataﬂow dynamically adjusts the PE mapping with a regrouping omega network to improve the operands reuse for higher hardware utilization. Third, a pointed-stake-shaped codec unit adaptively compresses small values to variable-length data format while compressing large values to ﬁxed-length 8b posit format, reducing the memory access for breaking the training energy bottleneck. Simulated with 28nm CMOS technology, the proposed PL-NPU achieves a maximum frequency of 1040MHz with 343mW and 5.28mm2. The peak energy efﬁciency is 3.87TFLOPS/W for 0.6V at 60MHz. Compared with the state-ofthe-art training processor, PL-NPU reaches 3.75× higher energy efﬁciency and offers 1.68× speedup when training ResNet18.},
	language = {en},
	number = {10},
	urldate = {2025-04-02},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Wang, Yang and Deng, Dazheng and Liu, Leibo and Wei, Shaojun and Yin, Shouyi},
	month = oct,
	year = {2022},
	pages = {4042--4055},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\C8UNSC68\\Wang et al. - 2022 - PL-NPU An Energy-Efficient Edge-Device DNN Training Processor With Posit-Based Logarithm-Domain Com.pdf:application/pdf},
}

@misc{noauthor_convolutional_nodate,
	title = {Convolutional {Neural} {Networks} ({CNNs}) explained - {YouTube}},
	url = {https://www.youtube.com/watch?v=YRhxdVk_sIs&ab_channel=deeplizard},
	urldate = {2025-04-03},
	file = {Convolutional Neural Networks (CNNs) explained - YouTube:C\:\\Users\\iamkr\\Zotero\\storage\\HQDV2NUX\\watch.html:text/html},
}

@article{beasley_developing_nodate,
	title = {Developing and {Implementing} {Dynamic} {Partial} {Reconfiguration} for {Pre}-{Emptible} {Context} {Switching} and {Continuous} {End}-{To}-{End} {Dataflow} {Applications}},
	abstract = {This study explores the benefits of the Dynamic Partial Reconfiguration (DPR) on Field Programmable Gate Array (FPGA) based System on Chip (SoC) architectures. Consideration is given to the constraints imposed by the implementation of partial reconfiguration on both pre-emptible context switching and continuous end-to-end dataflow applications. Skeleton structure systems that permit the insertion and removal of ‘blocks’ into the overall FPGA floorplan have been developed. These can be reconfigured dynamically by the on chip system host even during data processing. In pre-emptible context switching maintaining the execution state of a design before switching away from it becomes of paramount importance; this work presents a new Pre-emptible Flip Flop (PFF) design that is used as a basis for a Task Specific Access Structure (TSAS) for FPGA designs and then proposes an algorithm to automate the insertion of these PFF’s into a synthesised design. Expanding into continuous dataflow application design allows the system host to re-route signals during the partial reconfiguration process and then re-establish the processing chain with the new configuration hence maintaining a continuous uninterrupted dataflow.},
	language = {en},
	author = {Beasley, Alex and Walker, Luke and Clarke, Dr Chris},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\9CGTQFWK\\Beasley et al. - Developing and Implementing Dynamic Partial Reconfiguration for Pre-Emptible Context Switching and C.pdf:application/pdf},
}

@inproceedings{seong_fpga_2023,
	address = {Jeju, Korea, Republic of},
	title = {{FPGA} {Implementation} of {Cycle}-{Reduced} {Diagonal} {Data} {Flow} {Systolic} {Array} for {Edge} {Device} {AI}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3503-2703-8},
	url = {https://ieeexplore.ieee.org/document/10396567/},
	doi = {10.1109/ISOCC59558.2023.10396567},
	abstract = {As deep-learning has been widely adopted, utilization of resource-constrained edge devices becomes important to relax communication costs and user data privacy between server and edge device. The systolic array is generally used in deep-learning accelerators. Recent studies of systolic arrays are trying to reduce the total computation times of deep-learning applications inference. However, computation cycles for one matrix multiplication are not reduced. This paper proposes the systolic array for edge device with reduced computation cycles. The proposed systolic array reduces computation cycles by changing data transmitting direction between processing elements (PEs). We implement our design on FPGA in 200MHz frequency. The simulated cycle reduction in the 8×8 PE array is 31.82\% without additional resources.},
	language = {en},
	urldate = {2025-05-09},
	booktitle = {2023 20th {International} {SoC} {Design} {Conference} ({ISOCC})},
	publisher = {IEEE},
	author = {Seong, Gyubin and Park, Jong Kang and Kim, Jong Tae},
	month = oct,
	year = {2023},
	pages = {99--100},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\C6XL89UA\\Seong et al. - 2023 - FPGA Implementation of Cycle-Reduced Diagonal Data Flow Systolic Array for Edge Device AI.pdf:application/pdf},
}

@inproceedings{noauthor_design_2008,
	address = {Busan},
	title = {A {Design} and {Simulation} for {Dynamically} {Reconfigurable} {Systolic} {Array}},
	isbn = {978-0-7695-3407-7},
	url = {https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=4681983},
	doi = {10.1109/iccit14748.2008},
	abstract = {Systolic array is known as an architecture that can process a large amount of data with high speed, by large scale parallel and pipeline processing. If dynamic reconfiguration of systolic array is realized, flexible circuit construction and reduction of circuit scale become possible, without sacrificing the processing speed.},
	language = {en},
	urldate = {2025-07-14},
	publisher = {IEEE},
	month = nov,
	year = {2008},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\2LV86M32\\2008 - 2008 Third International Conference on Convergence and Hybrid Information Technology.pdf:application/pdf},
}

@inproceedings{wei_jin_mapping_2008,
	address = {Niagara Falls, ON, Canada},
	title = {Mapping multiple algorithms into a reconfigurable systolic array},
	url = {http://ieeexplore.ieee.org/document/4564726/},
	doi = {10.1109/ccece.2008.4564726},
	abstract = {Systolic array is a well known VLSI architecture to achieve extensive parallel and pipelining computing. Many systolic designs have been reported. All are algorithm based, that is one design is only for solving one speciﬁc problem. In this paper, the special purpose systolic architecture has been extended into a reconﬁgurable one and a systematic design approach to mapping two or more algorithms into a single reconﬁgurable systolic array is presented. First multiple algorithms are mapped into a reconﬁgurable systolic array that is able to compute one algorithm at a time with proper control settings. Second the reconﬁgurable systolic array is extended by using time or space redundancy so that it can compute multiple algorithms simultaneously. In addition, the optimal mapping, which minimizes the total hardware cost and computation time, is explored and the necessary condition of the transformation for computing multiple problem instances is also proposed. According to this condition, the search space of ﬁnding the optimal mapping can be signiﬁcantly reduced.},
	language = {en},
	urldate = {2025-07-14},
	booktitle = {2008 {Canadian} {Conference} on {Electrical} and {Computer} {Engineering}},
	publisher = {IEEE},
	author = {{Wei Jin} and Zhang, Chang N. and {Hua Li}},
	month = may,
	year = {2008},
	note = {ISSN: 0840-7789},
	pages = {001187--001192},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\VEILZ34P\\Wei Jin et al. - 2008 - Mapping multiple algorithms into a reconfigurable systolic array.pdf:application/pdf},
}

@article{zhang_energy-efficient_2022,
	title = {An {Energy}-{Efficient} {Convolutional} {Neural} {Network} {Processor} {Architecture} {Based} on a {Systolic} {Array}},
	volume = {12},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/12/24/12633},
	doi = {10.3390/app122412633},
	abstract = {Deep convolutional neural networks (CNNs) have shown strong abilities in the application of artiﬁcial intelligence. However, due to their extensive amount of computation, traditional processors have low energy efﬁciency when executing CNN algorithms, which is unacceptable for portable devices with limited hardware cost and battery capacity, so designing a CNN-speciﬁc processor is necessary. In this paper, we propose an energy-efﬁcient CNN processor architecture for lightweight devices with a processing elements (PEs) array consisting of 384 PEs. Using the systolic array-based PE array, it realizes parallel operations between ﬁlter rows and between channels of output feature maps, supporting the acceleration of 3D convolution and fully connected computation with various parameters by conﬁguring internal instruction registers. The computing strategy based on the proposed systolic dataﬂow achieves less hardware overhead compared with other strategies, and the reuse of image values and weight values, which effectively reduce the power of memory access. A memory system with a multi-level storage structure combined with register ﬁle (RF) and SRAM is used in the proposed CNN processor, which further reduces the energy overhead of computing. The proposed CNN processor architecture has been veriﬁed on a ZC706 FPGA platform using VGG-16 based on the proposed image segmentation method, the evaluation results indicate that the peak throughput achieves 115.2 GOP/s consuming 3.801 W at 150 MHz, energy efﬁciency and DSP efﬁciency reaches 30.32 GOP/s/W and 0.26 GOP/s/DSP, respectively.},
	language = {en},
	number = {24},
	urldate = {2025-07-14},
	journal = {Applied Sciences},
	author = {Zhang, Chen and Wang, Xin’an and Yong, Shanshan and Zhang, Yining and Li, Qiuping and Wang, Chenyang},
	month = dec,
	year = {2022},
	note = {Publisher: MDPI AG},
	pages = {12633},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\NQEVW53A\\Zhang et al. - 2022 - An Energy-Efficient Convolutional Neural Network Processor Architecture Based on a Systolic Array.pdf:application/pdf},
}

@article{noauthor_systolic_nodate,
	title = {Systolic {Array} {Design} for {Efficient} {FPGA} {Implementation} of {CNN} {Accelerators}: {Power} and {Area} {Optimizations}},
	abstract = {Convolutional Neural Networks (CNNs) are widely used in image processing, object detection, and other machine learning applications due to their ability to extract features from data effectively. However, the high computational and memory demands of CNNs pose significant challenges for real-time ap plications, especially in resource constrained environments such as edge devices where power consumption is a critical factor. This project proposes the design and implementation of a power efficient heterogeneous systolic array architecture integrated with low-power techniques to accelerate CNN operations. The design aims to optimize resource utilization and throughput while reducing power consumption. By leveraging specialized processing elements (PEs) and efficient dataflow methods, the proposed architecture seeks to achieve substantial improvements in power efficiency and performance compared to traditional systolic arrays.},
	language = {en},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\HS7H6JAU\\Systolic Array Design for Efficient FPGA Implementation of CNN Accelerators Power and Area Optimiza.pdf:application/pdf},
}

@inproceedings{palacios_systolic_2025,
	title = {Systolic {Arrays} and {Structured} {Pruning} {Co}-design for {Efficient} {Transformers} in {Edge} {Systems}},
	url = {http://arxiv.org/abs/2411.10285},
	doi = {10.1145/3716368.3735158},
	abstract = {Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition and machine translation using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 44\% system-wide speedups due to structured pruning and quantization were measured, with only 1.4\% word error rate degradation on the standard LibriSpeech dataset.},
	language = {en},
	urldate = {2025-07-16},
	author = {Palacios, Pedro and Medina, Rafael and Rouas, Jean-Luc and Ansaloni, Giovanni and Atienza, David},
	month = jun,
	year = {2025},
	note = {arXiv:2411.10285 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Hardware Architecture},
	pages = {320--327},
	annote = {Comment: 8 pages, GLSVLSI'25},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\78JD5H7U\\Palacios et al. - 2025 - Systolic Arrays and Structured Pruning Co-design for Efficient Transformers in Edge Systems.pdf:application/pdf},
}

@article{sun_sense_2023,
	title = {Sense: {Model}-{Hardware} {Codesign} for {Accelerating} {Sparse} {CNNs} on {Systolic} {Arrays}},
	volume = {31},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1063-8210, 1557-9999},
	shorttitle = {Sense},
	url = {https://ieeexplore.ieee.org/document/10043636/},
	doi = {10.1109/tvlsi.2023.3241933},
	abstract = {Sparsity is an intrinsic property of convolutional neural networks (CNNs), worth exploiting for CNN accelerators. However, the extra processing involved comes with hardware overhead, resulting in only marginal profits for most architectures. Meanwhile, systolic arrays have become increasingly competitive on CNN acceleration for its high spatiotemporal locality and low hardware overhead. However, the irregularity of sparsity induces imbalanced workloads under the rigid systolic dataflow, causing performance degradation. Thus, this article proposed a systolic-array-based architecture, called Sense, for sparse CNN acceleration by model-hardware codesign, enabling large performance gains. To balance input feature map (IFM) and weight loads across the processing element (PE) array, we applied channel clustering to gather IFMs with approximate sparsity for array computation and codesigned a load-balancing weight pruning method to keep the sparsity ratio of each kernel at a certain value with little accuracy loss, improving PE utilization and overall performance. In addition, adaptive dataflow configuration was applied to determine the computing strategy based on the storage ratio of IFMs and weights, lowering 1.17×–1.8× dynamic random access memory (DRAM) access compared with Swallow and further reducing system energy consumption. The whole design was implemented on ZynqZCU102 with 200 MHz and performs at 471, 34, 53, and 191 image/s for AlexNet, VGG-16, ResNet-50, and GoogleNet, respectively. Compared with sparse systolic-array-based accelerators, Swallow, fusion-enabled systolic architecture (FESA), and SPOTS, Sense achieves 0.97×–2.18×, 1.3×–1.67×, and 0.94×–1.82× energy efficiency (image/J) on these CNNs, respectively.},
	language = {en},
	number = {4},
	urldate = {2025-07-22},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	author = {Sun, Wenhao and Liu, Deng and Zou, Zhiwei and Sun, Wendi and Chen, Song and Kang, Yi},
	month = apr,
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {470--483},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\RQ3AW6II\\Sun et al. - 2023 - Sense Model-Hardware Codesign for Accelerating Sparse CNNs on Systolic Arrays.pdf:application/pdf},
}

@article{molchanov_pruning_2017,
	title = {{PRUNING} {CONVOLUTIONAL} {NEURAL} {NETWORKS} {FOR} {RESOURCE} {EFFICIENT} {INFERENCE}},
	abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efﬁcient inference. We interleave greedy criteria-based pruning with ﬁnetuning by backpropagation—a computationally efﬁcient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to ﬁne-grained classiﬁcation tasks (Birds-200 and Flowers-102) relaying only on the ﬁrst order gradient information. We also show that pruning can lead to more than 10× theoretical reduction in adapted 3D-convolutional ﬁlters with a small drop in accuracy in a recurrent gesture classiﬁer. Finally, we show results for the largescale ImageNet dataset to emphasize the ﬂexibility of our approach.},
	language = {en},
	author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
	year = {2017},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\46EKF39U\\Molchanov et al. - 2017 - PRUNING CONVOLUTIONAL NEURAL NETWORKS FOR RESOURCE EFFICIENT INFERENCE.pdf:application/pdf},
}

@article{zhang_machine_2023,
	title = {Machine {Learning} {Hardware} {Design} for {Efficiency}, {Flexibility}, and {Scalability} [{Feature}]},
	volume = {23},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1531-636X, 1558-0830},
	url = {https://ieeexplore.ieee.org/document/10284562/},
	doi = {10.1109/MCAS.2023.3302390},
	language = {en},
	number = {3},
	urldate = {2025-08-03},
	journal = {IEEE Circuits and Systems Magazine},
	author = {Zhang, Jie-Fang and Zhang, Zhengya},
	year = {2023},
	pages = {35--53},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\2HV533UR\\Zhang and Zhang - 2023 - Machine Learning Hardware Design for Efficiency, Flexibility, and Scalability [Feature].pdf:application/pdf},
}

@misc{parashar_scnn_2017,
	title = {{SCNN}: {An} {Accelerator} for {Compressed}-sparse {Convolutional} {Neural} {Networks}},
	shorttitle = {{SCNN}},
	url = {http://arxiv.org/abs/1708.04485},
	doi = {10.48550/arXiv.1708.04485},
	abstract = {Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efﬁciency are critical for deployments of CNNs in a wide range of situations, especially mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efﬁciency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator applied during inference. Speciﬁcally, SCNN employs a novel dataﬂow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataﬂow facilitates efﬁcient delivery of those weights and activations to the multiplier array, where they are extensively reused. In addition, the accumulation of multiplication products are performed in a novel accumulator array. Our results show that on contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7× and 2.3×, respectively, over a comparably provisioned dense CNN accelerator.},
	language = {en},
	urldate = {2025-09-18},
	publisher = {arXiv},
	author = {Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W. and Dally, William J.},
	month = may,
	year = {2017},
	note = {arXiv:1708.04485 [cs]},
	keywords = {Computer Science - Hardware Architecture, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:C\:\\Users\\iamkr\\Zotero\\storage\\AJZNLATT\\Parashar et al. - 2017 - SCNN An Accelerator for Compressed-sparse Convolutional Neural Networks.pdf:application/pdf},
}
